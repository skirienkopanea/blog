---
layout: post
title:  "Machine Learning Summary"
date:   2021-08-27 00:51:00 +0200
categories: datascience
tags: CSE2510
---
{% include math.html %}
<!--more-->

*This is a summary of the CSE2510 Machine Learning subject*

# Table of Contents
- [Table of Contents](#table-of-contents)
  - [What is machine learning?](#what-is-machine-learning)
  - [Supervised learning](#supervised-learning)
  - [Terminology](#terminology)
    - [Machine **Learning Algorithms**](#machine-learning-algorithms)
    - [Task](#task)
    - [Experience (dataset)](#experience-dataset)
      - [Training set](#training-set)
      - [Test set](#test-set)
      - [Features](#features)
      - [Design matrix](#design-matrix)
    - [Performance](#performance)
  - [Machine Learning is soft and hard classification (probabilistic)](#machine-learning-is-soft-and-hard-classification-probabilistic)
    - [Machine Learning Pipeline](#machine-learning-pipeline)
    - [Non-probabilstic Example: Polynomial Curve Fitting (Linear Model)](#non-probabilstic-example-polynomial-curve-fitting-linear-model)
  - [Probability theory](#probability-theory)
    - [Product rule](#product-rule)
    - [Sum rule](#sum-rule)
  - [Decision theory](#decision-theory)

## What is machine learning?
* With machine learning we can automatize boring simple repetitive tasks such as identifying patterns in a picture and group the patterns found into labels (i.e. recognising if the picture has a dog, a cat, a fruit...)
* In ML the algorithm "**learns**" from a "**training**" **input** data set and it applies what it "learned" to **new data inputs** via "**generalisation**"
  * **Learning == training on data** (more on this later)
  * Formally, **generalisation == Coming to general conclusions from (a limited number of) specific observations**
    * Example is to think "Bayesian", guessing the gender of a random pool is a 50/50 chance, but if the (training) data is updated with a pool selected from a synchronised swimming class, then the chance of guessing the gender is skewed towards female.
    * This is what the teacher means when she says that **Prior knowledge = counting observations** (i.e. number of girls in a class) and that **Learning is counting**.
      * Usually the more (representative) data (examples) the more "counting" (counting in proportion to the total pool, thus probability estimation)
      * Machine learning is about **predicting through counting (historical) data and assigning a most likely outcome** (probability)
* In a nusthell: [Machine learning is "probablistic" classification](#machine-learning-is-soft-classification)

## Supervised learning
* CSE2510 (and most of applications of ML) focus on supervised learning, that is "**learning by example**":
  * Given input-output examples, (explicitly) determine input-output function (i.e. probability of belonging to a "label" (apple, dog, cat, woman):
    * General input-output function: \\(y = AX + b\\)
      * X = input = set of features = design matrix
      * A = weight or coefficients = size of the effect of x on prediction y
      * b = bias
    * Dataset with label for each training example. Learns the association between example and label.
    * Example: inputs are apples and the output is the probability of the apple being red (for a human it's rather obvious or infered by context (i.e. lighting) whether an apple is red or not, but a ML input-output function won't output binary but as we observed before in the gender example the probability of the apple being red)
  * If we use "good" training data, the function should be able to generalise to new and previously unseen (apple) examples
    * The data needs to be relevant, you can't train the red apple example with oranges, or with only red apples, you need to feed the training set with both green and red apples with already defined outcomes to let the algorithm learn the difference between those sets of pictures
* Alternatively, unsupervised learning does not use predefined outcome labels for the algorithm and the algorithm decides on its own how to group the inputs, i.e. an algorithm might group dogs and chimps in the same group while bald bodybuilders and hippopotamus in the same one because it decided to use hair/fur as a label feature rather than bipedalism.
* There are other types of ML but they are not discussed in this course

## Terminology
### Machine **Learning Algorithms**
* A machine **learning algorithm** is an algorithm that is able to learn from data. Formally defined in 1997 as:
  * A computer program is said to learn from **experience** (\\(E\\)) with respect to some class of **tasks** (\\(T\\)) when it's task **performance** (\\(P\\)) improves with (more) experience.

### Task
* **Learning** is our **means of attaining the ability** **to** successfuly **perform** a **task**
  * **Learning != the task**
  * I.e. a robot being able to walk with "walking" as the task:
    * We could (manually) program the robot to walk ([QWOP](https://www.youtube.com/watch?v=YbYOsE7JyXs) style)
    * Alternatively, we could just program the robot to **learn** how to walk (this being just `import minecraft.py` style in python as opposed to codying the biomechanics yourself...)
      * So ML is a lazy yet effective solution at the expense of consuming lots of CPU and trainig resources (and as being a bruteforce approach rather than an elegant "mathematical deduction proof-based" approach (it's induction instead))
* Machine learning tasks are usually described in terms of how the machine learning system should process an **example**
  * An example is a collection of **features** that have been quantitatively measured from some object or event (i.e. a picture of a cat) that we want the machine learning to process
  * We typically represent the example as a vector \\(x \in \mathcal{R}^n\\) where each entry \\(X_i\\) of the vector is a feature (such that colors and shapes of groups of pixels)
    * Formally, the features of an image are usually the values of the pixels in the image
* Many **tasks** can be solved with machine learning ML such as:
  * **Classification**:
    1. Take measurements (features) of the objects (examples) (training data)
    2. Plot each object (examples) (training data)
    3. Label (apple, cat...) each object (examples) and draw the decision boundary (training data)
       * Avoid overfitting the decision boundary to the training data as this will make it harder to predict the label of new data
    4. Predict label of new objects (test data)
  * **Regression**: i.e. Predict house prices, the weather
  * **Transcription**: Transcribe a relatively unstructured representation of some kind of data into discrete textual form (i.e. speech recognition like autogenerated subtitles)
  * More... (not covered in this course)

### Experience (dataset)
* This is the dataset to train the ML algorithm
* Dataset: Collection of many examples or objects
* May or may not be supervised, if it is, then each of the examples in the training dataset is provided with an explicit label

#### Training set
* The set of examples used to finetune the algorithm "parameters"
  * At the end of the day, the function that returns the likeliness of an input to belong to a specific label is a function of the form \\(y=ax + by + cz...\\) with zillions of weights known as paramaters (well, or just use the mattrix notation)

#### Test set
* Independent from the test set, thus it is used exlusively to determine whether the finetune of the paramaters was accurate or not (formally known as **generasiability** of the trained model to unseen data).
* It must come from the same pool (same probability distribution as the training set)
* Goal of training: Learn a function that can predict a label y for a new x with as little error as possible = an input-output function that can generalise to new, unseen examples (without labels)
  * Learn model parameters a and b so that the error  of   the functionâ€™s predictions  is    minimised y =  ax + b 

#### Features
* Each data point/example/object is described in terms of features (i.e. shape, color, length, width of something in image)
* Features give a specific view of the objects: YOU (the user) are responsible for it
* Good features allow for pattern recognition, bad features allow for nothing
  * Thus more is not always better

#### Design matrix
* This matrix is just a way of describing a dataset
* Recall that each observation/example/object was regarded as a vector \\(x \in \mathcal{R}^n\\) where each entry \\(X_i\\) of the vector is a feature
* The design matrix \\(X\\) is a matrix that contains a different observation in each row, each column of the matrix corresponds to a different feature
  * This is expressed as \\(X \in \mathcal{R}^{\text{m}\times \text{n}}\\)
    * with m = number of observations and n = number of features

![404]({{ site.url }}/images/ml/design_matrix.PNG)

* See that \\(X_{i,0}\\) is the label, \\(X_{i,n}\\) are the features with \\(n>0\\) and the task: classify the penguins into
3 species based on the measurements (= features)
  * In unsupservised learning the label is not provided in the training set and the dataset is divided into clusters of "similar" objects 

### Performance
* Usually this performance measure is specific to the task, but we often just use the **accuracy** of the model (i.e. to predict the correct label)
  * The 1 - error rate is equivalent. The error rate is also regarded as "the expected 0-1 loss"
* Other performance measures include arbitrary weights for true/false positives/negatives (i.e. we might prefer to avoid false positives over maximizing true positives in insurance fraud detection, a true positive saves you 10k on average but a false positive can sue you on court and you lose 1 million and get bad reputation)
* Different ML techniques require different performance measures.

## Machine Learning is soft and hard classification (probabilistic)
* ML learning (algorithms whose performance improves with experience) regards "soft" classication tasks when probabilities to belong to a label are assigned with **conditional probabilities** based on **Baye's rule** (previous facts update the probability)
  * This is covered in Probability theory
* ML uses "hard" classification when establishing decision boundaries (percentage thershold that divides belonging to a label or not)
  * This is covered in decision theory
* The combination is called probabilistic classification, which is done using one or multiple ML techniques. This course covers:
  * Support vector machines
  * Linear regression
  * Neural networks
* No free-lunch theorem: no machine learning algorithm is better than any other as every classification algorithm has 
the same error rate for classifying new data. However, each algorithm is suitable for a different set of resources available by the researcher and for a specific task.

### Machine Learning Pipeline
* Regardless of the technique:

![404]({{ site.url }}/images/ml/pipeline.PNG)

* Factors that determine Performance:
  * The ML algorithmâ€™s ability to Make the **training error** small 
  * The ML algorithmâ€™s ability to Make the **gap between training and test error small**

### Non-probabilstic Example: Polynomial Curve Fitting (Linear Model)
* Regression problem used to introduce some terms. But note that this is not generally the correct ML approach.
* Suppose we observe a float `input variable` `x` and we want to use this input variable x to predict the value of another float `target variable` `t`
  * Secretly \\(t(x)=\sin{(2\pi x)} + b\\) with `b` being some random noise and the goal of the exercise is to identify a function that approxamites the real \\(t(x)\\).
* We're given a training set comprising N observations of `x` together with their corresponding `t`
  * This may be written as tuples in set theory notation: \\(N=\\{(x_1,t_1),(x_2,t_2),\dots,(x_n,t_n)\\}\\)
  * Or it could be written seperately as:
    * \\(\mathbf{x}\equiv (x_1,\dots,x_n)^T\\)
    * \\(\mathbf{t}\equiv (t_1,\dots,t_n)^T\\)
* Figure below shows a plot of a training set comprising N = 10 data points (blue circles) \\((x_i,t_i)\\)
* The green curve shows the function \\(t(x)=\sin{(2\pi x)}\\)  (without the noise) that implicitly applies to the observed the data.
  * Our goal is to predict the value of new \\(\hat{t}\\) for some new value of \\(\hat{x}\\) (hat denoting new data set) different from training set), without knowledge of the green curve.
    * This implicitly involves trying to discover the underlying function \\(\hat{t}(x)=\sin{(2\pi x)}\\) 

![404]({{ site.url }}/images/ml/regression.png)

* The real observed data (and very likely the new one as well) are corrupted with noise, and so for a given \\(\hat{x}\\) there is uncertainty as to the appropriate value for \\(\hat{t}\\)
  * [Probability theory](#probability-theory) provides a framework for expressing such uncertainty in a precise  and quantitative manne
  * [Decision theory](#decision-theory) allows us to exploit this probabilistic representation in order to make predictions that are optimal

* For the moment, however, we shall proceed rather informally and consider a simple approach based on curve fitting.
* In particular, we shall fit the data using a polynomial function of the form (note that \\(t\approx y\\)):
  * \\(y(x,\mathbf{x})=w_0 + w_1 x + w_2x^2 + \dots + w_n x^n\\)
  * $$y(x,\mathbf{x})=\sum^{n}_{i=0}{w_ix^i}$$
* This polynomial to approximate \\(\hat{t}(x)\\) is an approximation to a known calculus series (series are infinite)
  * sequences are infinite list of numbers written in a deffinite order i.e. \\(f(n) = f(n-2)+f(n-1) \text{ for } n\le 3\\)
  * Since a sequence is a function whose domain is the set of integers, its graph consists of discrete point coordinates.
![404]({{ site.url }}/images/ml/fibonacci.png)
  * series are the sum of a sequence of numbers (they're also discrete)
    * A series of the form \\(w_0 + w_1(x-a) + w_2(x-a)^2 + \dots\\) are called a power series
    * Any function f for which all derivatives exist in some point (such as a continous function like sin(x)) can be expressed as a power series where
      * \\(w_0 = f(a)/0!=f(a)\\)
      * \\(w_1 = f'(a)/1!\\)
      * \\(w_2 = f''(a)/2!\\)
      * \\(w_3 = f'''(a)/3!\\)
      * ...
    * These are known as [tylor series](https://www.youtube.com/watch?v=3d6DsjIBzJ4)
* Taylor polynomials are "finite tylor series" that better approximate the function f as \\(M\\) increases (degree of the polynomial)
  * $$\sum^M_{i=0}{\frac{f^{(i)}(a)}{i!}(x-a)^i}$$
  * where \\(f^{(i)}(a)\\) denotes the ith derivative of f evaluated at the point a.
* We do not know which function  our taylor polinomial is supposed to approximate but we will see that we can come up with a linear model that regards the selection of weights and then with an error function for those weights that can be minimized into a single solution.
* The weights (polynomial coefficients) \\(w_0, w_1, \dots, w_n\\) are collectively denoted by the vector \\(\mathbf{w}\\) although in statistics they are the vector \\(\beta\\) and X is the matrix that represents the polynomial powers (and \\(\epsilon\\) is noise) such that \\(y=X\beta + \epsilon\\) and:

$$ y = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\y_n\end{bmatrix}, X=\begin{bmatrix} 1 & x_1 & x^2_1\\ 1 & x_2 & x^2_2\\ \vdots & \vdots \\ 1 & x_n & x^2_n\end{bmatrix}, \beta=\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2\end{bmatrix}, \epsilon = \begin{bmatrix} \epsilon_0 \\ \epsilon_1 \\ \epsilon_2\end{bmatrix}$$
  
* In the CSE2510 this is expressed as \\(y(x, \mathbf{w}) = X\mathbf{w} + b\\)
* Although the function having various powers of x makes it implicitly nonlinear, it immedieately becomes a linear algebra equation once the x values have been inserted and we just then have \\(y = constantMatrix * vector + ErrorVector\\), which is reduced to a system of linear equations with n unkowns and n equations, which in matrix notation is just solving for \\(A\mathbf{x}=b\\).
  * This is formally known as a linear model
  * From linear algebra we know that a system of equations either has 0, 1 or infinite solutions
  * It is very likely that the tailor polynomial won't solve the system (the polynomial degree is far from \\(\infty\\) to approximate y "perfectly" and we secretly know that there was `b` random noise added deliberately)
* The values of the coefficients \\(\mathbf{w}\\) will be determined by fitting the polynomial to the training data
  * Since there's very likely no solution, we will just minimize the **error function** that measures the misfit between the model function \\(y(x,\mathbf{w})\\) and the actual training set data points (see that we're not using the hat notation as we are actually using training data and not new data)
  * One simple and popular choice of error function is given by
the sum of the squares of the errors between the approximation model \\(y(x,\mathbf{w}\\)) for each data point \\(x_j\\) and the actual target value \\(t_j\\).
  * $$E(\mathbf{w})=\frac{1}{2}\sum^n_{i=0}(y(x_i,\mathbf{w})-t_i)^2$$
  * Squaring takes cares of "absolutizing" the negative differences and halving the error is just for convinience, it doesn't change the final solution anyway
  * The equation pretty much gives the error size of a given set of polynomial weights (vector w).
![404]({{ site.url }}/images/ml/error.png)
  * The error function corresponds to (one half of) the sum of the squares of the displacements (shown by the vertical green bars) of each data point from the function \\(y(x,\mathbf{w})\\).
  * E(x) = 0 if and only if the function \\(y(x,\mathbf{w})\\) passes exactly through each training data point
* Because the error function is a quadratic function of the coefficients w, its derivatives with respect to the coefficients will be linear in the elements of w, and so the minimization of the error function has a unique solution, denoted by w\*, with the resulting polynomial given by the function \\(y(x,\mathbf{w^*})\\)
  * We can actually solve the least square solution \\(\mathbf{w^*}\\) with the so called "normal equation" of linear algebra (instead of calculating a long deriviative).
    * $$X^TX\mathbf{w^*}=X^Ty$$
    * This requires X to be invertible, if it isn't you can instead project y  onto the columnsspace of X and then solve for \\(Xw=\hat{y}\\)
* There remains the problem of choosing the order M of the polynomial, as we see below, a taylor ponimial with a high a low degree and a high degree produce poor results (note that the higher the degree of the polynomial, the more columns X has)
![404]({{ site.url }}/images/ml/degree.png)
  * Low order polynomial fit the training data very bad and we can clearly infer that they'll approximate new data very poorly as well
  * In the example below we see that M=3 produces close results for the training data but also for new data.
    * The goal is to achieve good generalization by making accurate predictions for new data, therefore we might be inclined to choose M=3. Altough it seems paradoxical. After all, the best solution would be \\(y=sin(2\pi x)\\) itself.
  * High order polynomials fit the training data perfectly, in fact M=9 has 0 error. But we can see that once new data is evaluated the model (red line) will give many errors. This is known as **over-fitting**
    * The reason it was possible to achieve E(x)=0 with M=9 is that this polynomial contained 10 degree of freedom corresponding to the 10 weight coefficients, which can be tuned exactly to fit the 10 data points in the training set.
    * The problem is that the polynomial is tunning too much to the random errors on the target values of the training set
    * For a given model complexity, the over-fitting problem becomes less severe as the size of the training data set increases, we can see that the higher order polynomial get's closer to \\(y=sin(2\pi x)\\) than M=3 did
    ![404]({{ site.url }}/images/ml/training.png)  
    * This follows the machine learning nature: an algorithm gets better with more training data
    * One rough heuristic that is sometimes advocated is that the number of data points should be no less than some multiple (say 5 or 10) of the number of adaptive parameters in the model (i.e. the taylor polynomial degree)
    * However there's no need to use thumbrules. By adopting a Bayesian approach, over-fitting problem can be avoided. There is no difficulty from a Bayesian perspective in employing models for which the number of parameters greatly exceeds the number of data points
      * In a Bayesian model the effective number of parameters adapts automatically to the size of the data set

## Probability theory
### Product rule
* $$p(X,Y)=p(Y|X)p(X)$$
  * "Probability of X and Y equals the probability of Y given X, times (the marginal) probability of X" (this is a rewritting of Bayes rule)

### Sum rule
* Example let S = sum of 2 dice, M = max of 2 dice
  * P(S=3,M=2) would be 2 scenarios ((1,2),(2,1)) of ouf the 6 x 6 possible, so \\(p_{S,M}(3,2)=2/36\\)

* From this joint probability mass function of \\(p_{S,M}(a,b)\\) we can derive the individual proabilities of \\(p_S(a)\\) and \\(p_M(b)\\) respectively: i.e. \\(p_S(6)=P(S=6)=P(S=6,b_1)+P(S=6,b_2)+\dots+P(6=b_j)\\).
* $$p_S(a)=\sum_{all\ b}{p_{S,M}(S=a,b_i)}$$

![jointandmarginal]({{ site.url }}/images/ml/jointandmarginal.PNG)

## Decision theory
* Yada

