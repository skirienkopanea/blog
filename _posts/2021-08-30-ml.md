---
layout: post
title:  "Machine Learning Summary"
date:   2021-08-27 00:51:00 +0200
categories: datascience
tags: CSE2510
---
{% include math.html %}
<!--more-->

*This is a summary of the CSE2510 Machine Learning subject*

# Table of Contents
- [Table of Contents](#table-of-contents)
  - [What is machine learning?](#what-is-machine-learning)
  - [Supervised learning](#supervised-learning)
  - [Terminology](#terminology)
    - [Machine **Learning Algorithms**](#machine-learning-algorithms)
    - [Task (i.e. collecting measurements (features) of examples (objects))](#task-ie-collecting-measurements-features-of-examples-objects)
    - [Experience (dataset)](#experience-dataset)
      - [Training set](#training-set)
      - [Test set](#test-set)
      - [Features](#features)
      - [Design matrix](#design-matrix)
    - [Performance](#performance)
  - [Learning](#learning)
    - [Machine Learning Pipeline](#machine-learning-pipeline)

## What is machine learning?
* With machine learning we can automatize boring simple repetitive tasks such as identifying patterns in a picture and group the patterns found into labels (i.e. recognising if the picture has a dog, a cat, a fruit...)
* In ML the algorithm "**learns**" from a "**training**" **input** data set and it applies what it "learned" to **new data inputs** via "**generalisation**"
  * **Learning == training on data** (more on this later)
  * Formally, **generalisation == Coming to general conclusions from (a limited number of) specific observations**
    * Example is to think "Bayesian", guessing the gender of a random pool is a 50/50 chance, but if the (training) data is updated with a pool selected from a synchronised swimming class, then the chance of guessing the gender is skewed towards female.
    * This is what the teacher means when she says that **Prior knowledge = counting observations** (i.e. number of girls in a class) and that **Learning is counting**.
      * Usually the more (representative) data (examples) the more "counting" (counting in proportion to the total pool, thus probability estimation)
      * Machine learning is about **predicting through counting (historical) data and assigning a most likely outcome** (probability)

## Supervised learning
* CSE2510 (and most of applications of ML) focus on supervised learning, that is "**learning by example**":
  * Given input-output examples, (explicitly) determine input-output function (i.e. probability of belonging to a "label" (apple, dog, cat, woman):
    * General input-output function: \\(y = AX + b\\)
      * X = input = set of features = design matrix
      * A = weight or coefficients = size of the effect of x on prediction y
      * b = bias
    * Dataset with label for each training example. Learns the association between example and label.
    * Example: inputs are apples and the output is the probability of the apple being red (for a human it's rather obvious or infered by context (i.e. lighting) whether an apple is red or not, but a ML input-output function won't output binary but as we observed before in the gender example the probability of the apple being red)
  * If we use "good" training data, the function should be able to generalise to new and previously unseen (apple) examples
    * The data needs to be relevant, you can't train the red apple example with oranges, or with only red apples, you need to feed the training set with both green and red apples with already defined outcomes to let the algorithm learn the difference between those sets of pictures
* Alternatively, unsupervised learning does not use predefined outcome labels for the algorithm and the algorithm decides on its own how to group the inputs, i.e. an algorithm might group dogs and chimps in the same group while bald bodybuilders and hippopotamus in the same one because it decided to use hair/fur as a label feature rather than bipedalism.
* There are other types of ML but they are not discussed in this course

## Terminology
### Machine **Learning Algorithms**
* A machine **learning algorithm** is an algorithm that is able to learn from data. Formally defined in 1997 as:
  * A computer program is said to learn from **experience** (\\(E\\)) with respect to some class of **tasks** (\\(T\\)) when it's task **performance** (\\(P\\)) improves with (more) experience.

### Task (i.e. collecting measurements (features) of examples (objects))
* **Learning** is our **means of attaining the ability** **to** successfuly **perform** a **task**
  * **Learning != the task**
  * I.e. a robot being able to walk with "walking" as the task:
    * We could (manually) program the robot to walk ([QWOP](https://www.youtube.com/watch?v=YbYOsE7JyXs) style)
    * Alternatively, we could just program the robot to **learn** how to walk (this being just `import minecraft.py` style in python as opposed to codying the biomechanics yourself...)
      * So ML is a lazy yet effective solution at the expense of consuming lots of CPU and trainig resources (and as being a bruteforce approach rather than an elegant "mathematical deduction proof-based" approach (not that stats != math))
* Machine learning tasks are usually described in terms of how the machine learning system should process an **example**
  * An example is a collection of **features** that have been quantitatively measured from some object or event (i.e. a picture of a cat) that we want the machine learning to process
  * We typically represent the example as a vector \\(x \in \mathcal{R}^n\\) where each entry \\(X_i\\) of the vector is a feature (such that colors and shapes of groups of pixels)
    * Formally, the features of an image are usually the values of the pixels in the image
* Many **tasks** can be solved with machine learning ML such as:
  * **Classification**:
    1. Take measurements (features) of the objects (pictures) (training data)
    2. Plot each object (picture) (training data)
    3. Label (apple, cat...) each object (picture) and draw the decision boundary (training data)
       * Avoid overfitting the decision boundary to the training data as this will make it harder to predict the label of new data
    4. Predict label of new objects (test data)
  * **Regression**: i.e. Predict house prices, the weather
  * **Transcription**: Transcribe a relatively unstructured representation of some kind of data into discrete textual form (i.e. speech recognition like autogenerated subtitles)
  * More... (not covered in this course)

### Experience (dataset)
* This is the dataset to train the ML algorithm
* Dataset: Collection of many examples or objects
* May or may not be supervised, if it is, then each of the examples in the training dataset is provided with an explicit label

#### Training set
* The set of examples used to finetune the algorithm "parameters"
  * At the end of the day, the function that returns the likeliness of an input to belong to a specific label is a function of the form \\(y=ax + by + cz...\\) with zillions of weights known as paramaters (well, or just use the mattrix notation)

#### Test set
* Independent from the test set, thus it is used exlusively to determine whether the finetune of the paramaters was accurate or not (formally known as **generasiability** of the trained model to unseen data).
* It must come from the same pool (same probability distribution as the training set)
* Goal of training: Learn a function that can predict a label y for a new x with as little error as possible = an input-output function that can generalise to new, unseen examples (without labels)
  * Learn model parameters a and b so that the error  of   the functionâ€™s predictions  is    minimised y =  ax + b 

#### Features
* Each data point/example/object is described in terms of features (i.e. shape, color, length, width of something in image)
* Features give a specific view of the objects: YOU (the user) are responsible for it
* Good features allow for pattern recognition, bad features allow for nothing
  * Thus more is not always better

#### Design matrix
* This matrix is just a way of describing a dataset
* Recall that each observation/example/object was regarded as a vector \\(x \in \mathcal{R}^n\\) where each entry \\(X_i\\) of the vector is a feature
* The design matrix \\(X\\) is a amtrix that contains a different observation in each row, each column of the matrix corresponds to a different feature
  * This is expressed as \\(X \in \mathcal{R}^{\text{m}\times \text{n}}\\)
    * with m = number of observations and n = number of features

![404]({{ site.url }}/images/ml/design_matrix.PNG)

* See that \\(X_{i,0}\\) is the label, \\(X_{i,n}\\) are the features with \\(n>0\\) and the task: classify the penguins into
3 species based on the measurements (= features)
  * In unsupservised learning the label is not provided in teh training set and the dataset is divided into clusters of "similar" objects 

### Performance
* Usually this performance measure is specific to the task, but we often just use the **accuracy** of the model (i.e. to predict the correct label)
  * The 1 - error rate is equivalent. The error rate is also regarded as "the expected 0-1 loss"
* Other performance measures include arbitrary weights for true/false positives/negatives (i.e. we might prefer to avoid false positives over maximizing true positives in insurance fraud detection, a true positive saves you 10k on average but a false positive can sue you on court and you lose 1 million and get bad reputation)
* Different ML techniques require different performance measures.

## Learning
* We've seen that instead of harcoding QWOP, we just want to let the machine "learn" by itself, but it's not as simple as just writting `import learn.py`

* Learning is done using one or multiple ML techniques. This course covers:
  * Support vector machines
  * Linear regression
  * Neural networks

### Machine Learning Pipeline
* Regardless of the technique:

![404]({{ site.url }}/images/ml/pipeline.PNG)

* Factors that determine Performance:
  * The ML algorithmâ€™s ability to Make the **training error** small 
  * The ML algorithmâ€™s ability to Make the **gap between training and test error small**

* No free-lunch theorem: no machine learning algorithm is better than any other as every classification algorithm has 
the same error rate for classifying new data. However, each algorithm is suitable for a different set of resources available by the researcher and for a specific task.

* In a nutshell: Machine learning is just learning patterns in data to be able to carry out a repetitive task