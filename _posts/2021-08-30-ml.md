---
layout: post
title:  "Machine Learning Summary"
date:   2021-08-27 00:51:00 +0200
categories: datascience
tags: CSE2510
---
{% include math.html %}
<!--more-->

*This is a summary of the CSE2510 Machine Learning subject*

# Table of Contents
- [Table of Contents](#table-of-contents)
  - [What is machine learning?](#what-is-machine-learning)
  - [Note on notation](#note-on-notation)
  - [Supervised learning](#supervised-learning)
  - [Terminology](#terminology)
    - [Machine **Learning Algorithms**](#machine-learning-algorithms)
    - [Task](#task)
    - [Experience (dataset)](#experience-dataset)
      - [Training set](#training-set)
      - [Test set](#test-set)
      - [Features](#features)
      - [Design matrix](#design-matrix)
    - [Performance](#performance)
  - [Machine Learning is soft and hard classification (probabilistic)](#machine-learning-is-soft-and-hard-classification-probabilistic)
    - [Machine Learning Pipeline](#machine-learning-pipeline)
  - [Non-probabilstic example: Polynomial Curve Fitting (Linear Model)](#non-probabilstic-example-polynomial-curve-fitting-linear-model)
  - [Probability theory in machine learning](#probability-theory-in-machine-learning)
    - [Discrete random variable](#discrete-random-variable)
    - [Joint probability](#joint-probability)
    - [Sum rule](#sum-rule)
    - [Conditional probability (and product rule)](#conditional-probability-and-product-rule)
    - [Fundamental probability rules](#fundamental-probability-rules)
    - [Bayes rule](#bayes-rule)
    - [Relation to ML](#relation-to-ml)
    - [Conditional probablity for continous variables](#conditional-probablity-for-continous-variables)
  - [Decision theory](#decision-theory)
    - [Decision boundary](#decision-boundary)
    - [Soft-Hard classification process recap and complicated decision boundaries](#soft-hard-classification-process-recap-and-complicated-decision-boundaries)
    - [Rewritting the classifier for convience](#rewritting-the-classifier-for-convience)
    - [Model](#model)
    - [Classification error](#classification-error)
    - [Missclassification costs](#missclassification-costs)
  - [Density-based classifiers](#density-based-classifiers)
    - [Estimating probability (density) functions](#estimating-probability-density-functions)
    - [Histogram-based density estimation](#histogram-based-density-estimation)
    - [Curse of dimensionality](#curse-of-dimensionality)
  - [Parametric density estimation](#parametric-density-estimation)
    - [Assume Gaussian (Bell shaped) distribution](#assume-gaussian-bell-shaped-distribution)
      - [Covariance matrix and mean vector in a 2D gaussian distribution](#covariance-matrix-and-mean-vector-in-a-2d-gaussian-distribution)
    - [Maximum likelihood estimates (for the mean and coveriance matrix)](#maximum-likelihood-estimates-for-the-mean-and-coveriance-matrix)
  - [Gausian density based classifiers](#gausian-density-based-classifiers)
    - [Two-Class case](#two-class-case)
    - [Fixing no inverse covariance matrix for Two-classes case](#fixing-no-inverse-covariance-matrix-for-two-classes-case)
    - [Linear discriminant](#linear-discriminant)
      - [Rewriting the bias term (\\(w_0\\))](#rewriting-the-bias-term-w_0)
  - [Nearest Mean Classifier (NMC)](#nearest-mean-classifier-nmc)
    - [Feature scaling](#feature-scaling)
  - [Model complexity and available data trade-off](#model-complexity-and-available-data-trade-off)
  - [Lab: univariate classifier in Pyton](#lab-univariate-classifier-in-pyton)

## What is machine learning?
* With machine learning we can automatize boring simple repetitive tasks such as identifying patterns in a picture and group the patterns found into labels (i.e. recognising if the picture has a dog, a cat, a fruit...)
* In ML the algorithm "**learns**" from a "**training**" **input** data set and it applies what it "learned" to **new data inputs** via "**generalisation**"
  * **Learning == training on data** (more on this later)
  * Formally, **generalisation == Coming to general conclusions from (a limited number of) specific observations**
    * Example is to think "Bayesian", guessing the gender of a random pool is a 50/50 chance, but if the (training) data is updated with a pool selected from a synchronised swimming class, then the chance of guessing the gender is skewed towards female.
    * This is what the teacher means when she says that **Prior knowledge = counting observations** (i.e. number of girls in a class) and that **Learning is counting**.
      * Usually the more (representative) data (examples) the more "counting" (counting in proportion to the total pool, thus probability estimation)
      * Machine learning is about **predicting through counting (historical) data and assigning a most likely outcome** (probability)
* In a nusthell: [Machine learning is "probablistic" classification](#machine-learning-is-soft-classification)

## Note on notation
* \\(p(x)\\) is used for continous variables
* \\(P(x)\\) is used for discrete variables
* In machine learning the feature vector x is often continous (i.e. weight, height) while the class labels are discrete (i.e. woman, man)
  * Therefore in machine learning we just use p(x) for both
* In ML it is arbitrary to store the measurements of the feature vector in rows (i.e. one objecte example per row) or in columns (one column contains 1 object example)
* In CSE2510 we store object in rows such that:
  * $$\begin{bmatrix} -1 & -1 \\ -1  & 1 \\ 2 & 0 \\ 3 & 0 \end{bmatrix}$$
  * Is a two-feature dataset with four objects

## Supervised learning
* CSE2510 (and most of applications of ML) focus on supervised learning, that is "**learning by example**":
  * Given input-output examples, (explicitly) determine input-output function (i.e. probability of belonging to a "label" (apple, dog, cat, woman):
    * General input-output function: \\(y = AX + b\\)
      * X = input = set of features = design matrix
      * A = weight or coefficients = size of the effect of x on prediction y
      * b = bias
    * Dataset with label for each training example. Learns the association between example and label.
    * Example: inputs are apples and the output is the probability of the apple being red (for a human it's rather obvious or infered by context (i.e. lighting) whether an apple is red or not, but a ML input-output function won't output binary but as we observed before in the gender example the probability of the apple being red)
  * If we use "good" training data, the function should be able to generalise to new and previously unseen (apple) examples
    * The data needs to be relevant, you can't train the red apple example with oranges, or with only red apples, you need to feed the training set with both green and red apples with already defined outcomes to let the algorithm learn the difference between those sets of pictures
* Alternatively, unsupervised learning does not use predefined outcome labels for the algorithm and the algorithm decides on its own how to group the inputs, i.e. an algorithm might group dogs and chimps in the same group while bald bodybuilders and hippopotamus in the same one because it decided to use hair/fur as a label feature rather than bipedalism.
* There are other types of ML but they are not discussed in this course

## Terminology
### Machine **Learning Algorithms**
* A machine **learning algorithm** is an algorithm that is able to learn from data. Formally defined in 1997 as:
  * A computer program is said to learn from **experience** (\\(E\\)) with respect to some class of **tasks** (\\(T\\)) when it's task **performance** (\\(P\\)) improves with (more) experience.

### Task
* **Learning** is our **means of attaining the ability** **to** successfuly **perform** a **task**
  * **Learning != the task**
  * I.e. a robot being able to walk with "walking" as the task:
    * We could (manually) program the robot to walk ([QWOP](https://www.youtube.com/watch?v=YbYOsE7JyXs) style)
    * Alternatively, we could just program the robot to **learn** how to walk (this being just `import minecraft.py` style in python as opposed to codying the biomechanics yourself...)
      * So ML is a lazy yet effective solution at the expense of consuming lots of CPU and trainig resources (and as being a bruteforce approach rather than an elegant "mathematical deduction proof-based" approach (it's induction instead))
* Machine learning tasks are usually described in terms of how the machine learning system should process an **example**
  * An example is a collection of **features** that have been quantitatively measured from some object or event (i.e. a picture of a cat) that we want the machine learning to process
  * We typically represent the example as a vector \\(x \in \mathcal{R}^n\\) where each entry \\(X_i\\) of the vector is a feature (such that colors and shapes of groups of pixels)
    * Formally, the features of an image are usually the values of the pixels in the image
* Many **tasks** can be solved with machine learning ML such as:
  * **Classification**:
    1. Take measurements (features) of the objects (examples) (training data)
    2. Plot each object (examples) (training data)
    3. Label (apple, cat...) each object (examples) and draw the decision boundary (training data)
       * Avoid overfitting the decision boundary to the training data as this will make it harder to predict the label of new data
    4. Predict label of new objects (test data)
  * **Regression**: predicting a continous value (i.e. house prices)
  * **Transcription**: Transcribe a relatively unstructured representation of some kind of data into discrete textual form (i.e. speech recognition like autogenerated subtitles)
  * More... (not covered in this course)

### Experience (dataset)
* This is the dataset to train the ML algorithm
* Dataset: Collection of many examples or objects
* May or may not be supervised, if it is, then each of the examples in the training dataset is provided with an explicit label

#### Training set
* The set of examples used to finetune the algorithm "parameters"
  * At the end of the day, the function that returns the likeliness of an input to belong to a specific label is a function of the form \\(y=ax + by + cz...\\) with zillions of weights known as paramaters (well, or just use the mattrix notation)

#### Test set
* Independent from the test set, thus it is used exlusively to determine whether the finetune of the paramaters was accurate or not (formally known as **generasiability** of the trained model to unseen data).
* It must come from the same pool (same probability distribution as the training set)
* Goal of training: Learn a function that can predict a label y for a new x with as little error as possible = an input-output function that can generalise to new, unseen examples (without labels)
  * Learn model parameters a and b so that the error  of   the function’s predictions  is    minimised y =  ax + b 

#### Features
* Each data point/example/object is described in terms of features (i.e. shape, color, length, width of something in image)
* Features give a specific view of the objects: YOU (the user) are responsible for it
* Good features allow for pattern recognition, bad features allow for nothing
  * Thus more is not always better

#### Design matrix
* This matrix is just a way of describing a dataset
* Recall that each observation/example/object was regarded as a vector \\(x \in \mathcal{R}^n\\) where each entry \\(X_i\\) of the vector is a feature
* The design matrix \\(X\\) is a matrix that contains a different observation in each row, each column of the matrix corresponds to a different feature
  * This is expressed as \\(X \in \mathcal{R}^{\text{m}\times \text{n}}\\)
    * with m = number of observations and n = number of features

![404]({{ site.url }}/images/ml/design_matrix.PNG)

* See that \\(X_{i,0}\\) is the label, \\(X_{i,n}\\) are the features with \\(n>0\\) and the task: classify the penguins into
3 species based on the measurements (= features)
  * In unsupservised learning the label is not provided in the training set and the dataset is divided into clusters of "similar" objects 

### Performance
* Usually this performance measure is specific to the task, but we often just use the **accuracy** of the model (i.e. to predict the correct label)
  * The 1 - error rate is equivalent. The error rate is also regarded as "the expected 0-1 loss"
* Other performance measures include arbitrary weights for true/false positives/negatives (i.e. we might prefer to avoid false positives over maximizing true positives in insurance fraud detection, a true positive saves you 10k on average but a false positive can sue you on court and you lose 1 million and get bad reputation)
* Different ML techniques require different performance measures.

## Machine Learning is soft and hard classification (probabilistic)
* ML learning (algorithms whose performance improves with experience) regards "soft" classication tasks when probabilities to belong to a label are assigned with **conditional probabilities** based on **Baye's rule** (previous facts update the probability)
  * This is covered in Probability theory
* ML uses "hard" classification when establishing decision boundaries (percentage thershold that divides belonging to a label or not)
  * This is covered in decision theory
* The combination is called probabilistic classification, which is done using one or multiple ML techniques. This course covers:
  * Support vector machines
  * Linear regression
  * Neural networks
* No free-lunch theorem: no machine learning algorithm is better than any other as every classification algorithm has 
the same error rate for classifying new data. However, each algorithm is suitable for a different set of resources available by the researcher and for a specific task.

### Machine Learning Pipeline
* Regardless of the technique:

![404]({{ site.url }}/images/ml/pipeline.PNG)

* Factors that determine Performance:
  * The ML algorithm’s ability to Make the **training error** small 
  * The ML algorithm’s ability to Make the **gap between training and test error small**

![404]({{ site.url }}/images/ml/pipeline2.PNG)

## Non-probabilstic example: Polynomial Curve Fitting (Linear Model)
* Regression problem used to introduce some terms. But note that this is not generally the correct ML approach.
* Suppose we observe a float `input variable` `x` and we want to use this input variable x to predict the value of another float `target variable` `t`
  * Secretly \\(t(x)=\sin{(2\pi x)} + b\\) with `b` being some random noise and the goal of the exercise is to identify a function that approxamites the real \\(t(x)\\).
* We're given a training set comprising N observations of `x` together with their corresponding `t`
  * This may be written as tuples in set theory notation: \\(N=\\{(x_1,t_1),(x_2,t_2),\dots,(x_n,t_n)\\}\\)
  * Or it could be written seperately as:
    * \\(\mathbf{x}\equiv (x_1,\dots,x_n)^T\\)
    * \\(\mathbf{t}\equiv (t_1,\dots,t_n)^T\\)
* Figure below shows a plot of a training set comprising N = 10 data points (blue circles) \\((x_i,t_i)\\)
* The green curve shows the function \\(t(x)=\sin{(2\pi x)}\\)  (without the noise) that implicitly applies to the observed the data.
  * Our goal is to predict the value of new \\(\hat{t}\\) for some new value of \\(\hat{x}\\) (hat denoting new data set) different from training set), without knowledge of the green curve.
    * This implicitly involves trying to discover the underlying function \\(\hat{t}(x)=\sin{(2\pi x)}\\) 

![404]({{ site.url }}/images/ml/regression.png)

* The real observed data (and very likely the new one as well) are corrupted with noise, and so for a given \\(\hat{x}\\) there is uncertainty as to the appropriate value for \\(\hat{t}\\)
  * [Probability theory](#probability-theory) provides a framework for expressing such uncertainty in a precise  and quantitative manne
  * [Decision theory](#decision-theory) allows us to exploit this probabilistic representation in order to make predictions that are optimal

* For the moment, however, we shall proceed rather informally and consider a simple approach based on curve fitting.
* In particular, we shall fit the data using a polynomial function of the form (note that \\(t\approx y\\)):
  * \\(y(x,\mathbf{x})=w_0 + w_1 x + w_2x^2 + \dots + w_n x^n\\)
  * $$y(x,\mathbf{x})=\sum^{n}_{i=0}{w_ix^i}$$
* This polynomial to approximate \\(\hat{t}(x)\\) is an approximation to a known calculus series (series are infinite)
  * sequences are infinite list of numbers written in a deffinite order i.e. \\(f(n) = f(n-2)+f(n-1) \text{ for } n\le 3\\)
  * Since a sequence is a function whose domain is the set of integers, its graph consists of discrete point coordinates.
![404]({{ site.url }}/images/ml/fibonacci.png)
  * series are the sum of a sequence of numbers (they're also discrete)
    * A series of the form \\(w_0 + w_1(x-a) + w_2(x-a)^2 + \dots\\) are called a power series
    * Any function f for which all derivatives exist in some point (such as a continous function like sin(x)) can be expressed as a power series where
      * \\(w_0 = f(a)/0!=f(a)\\)
      * \\(w_1 = f'(a)/1!\\)
      * \\(w_2 = f''(a)/2!\\)
      * \\(w_3 = f'''(a)/3!\\)
      * ...
    * These are known as [tylor series](https://www.youtube.com/watch?v=3d6DsjIBzJ4)
* Taylor polynomials are "finite tylor series" that better approximate the function f as \\(M\\) increases (degree of the polynomial)
  * $$\sum^M_{i=0}{\frac{f^{(i)}(a)}{i!}(x-a)^i}$$
  * where \\(f^{(i)}(a)\\) denotes the ith derivative of f evaluated at the point a.
* We do not know which function  our taylor polinomial is supposed to approximate but we will see that we can come up with a linear model that regards the selection of weights and then with an error function for those weights that can be minimized into a single solution.
* The weights (polynomial coefficients) \\(w_0, w_1, \dots, w_n\\) are collectively denoted by the vector \\(\mathbf{w}\\) although in statistics they are the vector \\(\beta\\) and X is the matrix that represents the polynomial powers (and \\(\epsilon\\) is noise) such that \\(y=X\beta + \epsilon\\) and:

$$ y = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\y_n\end{bmatrix}, X=\begin{bmatrix} 1 & x_1 & x^2_1\\ 1 & x_2 & x^2_2\\ \vdots & \vdots \\ 1 & x_n & x^2_n\end{bmatrix}, \beta=\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2\end{bmatrix}, \epsilon = \begin{bmatrix} \epsilon_0 \\ \epsilon_1 \\ \epsilon_2\end{bmatrix}$$
  
* In the CSE2510 this is expressed as \\(y(x, \mathbf{w}) = X\mathbf{w} + b\\)
* Although the function having various powers of x makes it implicitly nonlinear, it immedieately becomes a linear algebra equation once the x values have been inserted and we just then have \\(y = constantMatrix * vector + ErrorVector\\), which is reduced to a system of linear equations with n unkowns and n equations, which in matrix notation is just solving for \\(A\mathbf{x}=b\\).
  * This is formally known as a linear model
  * From linear algebra we know that a system of equations either has 0, 1 or infinite solutions
  * It is very likely that the tailor polynomial won't solve the system (the polynomial degree is far from \\(\infty\\) to approximate y "perfectly" and we secretly know that there was `b` random noise added deliberately)
* The values of the coefficients \\(\mathbf{w}\\) will be determined by fitting the polynomial to the training data
  * Since there's very likely no solution, we will just minimize the **error function** that measures the misfit between the model function \\(y(x,\mathbf{w})\\) and the actual training set data points (see that we're not using the hat notation as we are actually using training data and not new data)
  * One simple and popular choice of error function is given by
the sum of the squares of the errors between the approximation model \\(y(x,\mathbf{w}\\)) for each data point \\(x_j\\) and the actual target value \\(t_j\\).
  * $$E(\mathbf{w})=\frac{1}{2}\sum^n_{i=0}(y(x_i,\mathbf{w})-t_i)^2$$
  * Squaring takes cares of "absolutizing" the negative differences and halving the error is just for convinience, it doesn't change the final solution anyway
  * The equation pretty much gives the error size of a given set of polynomial weights (vector w).
![404]({{ site.url }}/images/ml/error.png)
  * The error function corresponds to (one half of) the sum of the squares of the displacements (shown by the vertical green bars) of each data point from the function \\(y(x,\mathbf{w})\\).
  * E(x) = 0 if and only if the function \\(y(x,\mathbf{w})\\) passes exactly through each training data point
* Because the error function is a quadratic function of the coefficients w, its derivatives with respect to the coefficients will be linear in the elements of w, and so the minimization of the error function has a unique solution, denoted by w\*, with the resulting polynomial given by the function \\(y(x,\mathbf{w^*})\\)
  * We can actually solve the least square solution \\(\mathbf{w^*}\\) with the so called "normal equation" of linear algebra (instead of calculating a long deriviative).
    * $$X^TX\mathbf{w^*}=X^Ty$$
    * This requires X to be invertible, if it isn't you can instead project y  onto the columnsspace of X and then solve for \\(Xw=\hat{y}\\)
* There remains the problem of choosing the order M of the polynomial, as we see below, a taylor ponimial with a high a low degree and a high degree produce poor results (note that the higher the degree of the polynomial, the more columns X has)
![404]({{ site.url }}/images/ml/degree.png)
  * Low order polynomial fit the training data very bad and we can clearly infer that they'll approximate new data very poorly as well
  * In the example below we see that M=3 produces close results for the training data but also for new data.
    * The goal is to achieve good generalization by making accurate predictions for new data, therefore we might be inclined to choose M=3. Altough it seems paradoxical. After all, the best solution would be \\(y=sin(2\pi x)\\) itself.
  * High order polynomials fit the training data perfectly, in fact M=9 has 0 error. But we can see that once new data is evaluated the model (red line) will give many errors. This is known as **over-fitting**
    * The reason it was possible to achieve E(x)=0 with M=9 is that this polynomial contained 10 degree of freedom corresponding to the 10 weight coefficients, which can be tuned exactly to fit the 10 data points in the training set.
    * The problem is that the polynomial is tunning too much to the random errors on the target values of the training set
    * For a given model complexity, the over-fitting problem becomes less severe as the size of the training data set increases, we can see that the higher order polynomial get's closer to \\(y=sin(2\pi x)\\) than M=3 did
    ![404]({{ site.url }}/images/ml/training.png)  
    * This follows the machine learning nature: an algorithm gets better with more training data
    * One rough heuristic that is sometimes advocated is that the number of data points should be no less than some multiple (say 5 or 10) of the number of adaptive parameters in the model (i.e. the taylor polynomial degree)
    * However there's no need to use thumbrules. By adopting a Bayesian approach, over-fitting problem can be avoided. There is no difficulty from a Bayesian perspective in employing models for which the number of parameters greatly exceeds the number of data points
      * In a Bayesian model the effective number of parameters adapts automatically to the size of the data set

## Probability theory in machine learning
* In the context of ML: we want to design functions that classify an
unknown object in the most likely class
* Our task is to determine what "most likely" is
  * For which we for any ML algorithm we will always use a form of conditional probablity:
    * $$p(y|x)$$
    * which is read as "Probability of y knowing that x happened" (formally: probability of y given x)
  * In the context of ML:
    * $$p(class|object)$$
    * "Probability of a particular object belonging to a particular class"
    * $$p(label|feature vector)$$
    * "Probability of an example being a specific label given a set of features"
    * Recall that the feature vector is how we describe an object
* Recall that all the tuples of \\((x,p(x))\\) are regarded as the "probability distribution" of x.
  * The joint probability distribution would be all the tuples of \\((x,y,p(x,y))\\)
* Our ML classifier algorithm function will therefore not return a black and white answer saying that "input belongs to class N", instead the algorithm function will return the probability of an input belonging to a class (i.e. when image recogintion have "90% dog" output).
* Only after having then outputted a probabilistic score, will we use decision theory to decide whether the score (probablity) is high enough to assign it to a lable, to not assign, or to leave it as "unclear".

### Discrete random variable
* Discrete (i.e. "int","enum" instead of "float"(kinda)) **random** variables are capitalized
  * each of the possible values that they can take are expressed in small cap (for enum) or the actual discrete number value they can take

![404]({{ site.url }}/images/ml/discrete.PNG)  

### Joint probability
* The joint probability of 2 random variables occuring can be infered graphically:

![404]({{ site.url }}/images/ml/joint.PNG)

* The joint probability is equal to the number \\(n_{ij}\\) of cases where both \\(x_i, y_j\\) occurred, divided by the total number of cases N (which is the number of cases from the cross product of X and Y possible values sets only when the objects of the sets have all the same probability, if not you have to count them manually yourself from the context)

### Sum rule
![404]({{ site.url }}/images/ml/sum.PNG)
* L = number of total possible y values

* Example let S = sum of 2 dice, M = max of 2 dice
  * P(S=3,M=2) would be 2 scenarios ((1,2),(2,1)) of ouf the 6 x 6 possible, so \\(p(S=3,M=2)=2/36\\)

* If we would just be given the joint probability mass function \\(p(S=a,M=b)\\) (inside cells) we can derive the individual proabilities of \\(p(S=a)\\) and \\(p(M=b)\\) respectively: i.e. \\(P(S=6)=P(S=6,M=b_1)+P(S=6,M=b_2)+\dots+P(S=6,M=b_L)\\).

![404]({{ site.url }}/images/ml/jointandmarginal.PNG)

* Marginal distributions are those on the borders and the joint distribution is made from the inside cells.
* With the joint, we can always find the marginals (and conditionals in all orders), but with the marginals we cannot always find the joint (nor the conditionals)

### Conditional probability (and product rule)
* for p(y\|x) it's the number of cases where y and x holds, divided by the number of cases where x holds
![404]({{ site.url }}/images/ml/conditional.PNG)
* If you divide both the numerator and the denominator by N each, then you actually get:
  * $$p(y|x)=\frac{p(y,x)}{p(x)}$$
  * $$p(y,x)=p(y|x)p(x)$$
  * Which is the product rule

### Fundamental probability rules
![404]({{ site.url }}/images/ml/rules.PNG)

### Bayes rule
![404]({{ site.url }}/images/ml/bayes.PNG)
* The power behind bayes rule is that while P(Y\|X) might not be given, we can on our own find P(X\|Y) as well as the marginal probabilties.
  * Note that bayes rule is just a conditional probability rule whose joint probability term has been replaced with the product rule equivalent
* With that we can calculate the conditional probability of an object belonging to a label, which is basically the solution for the machine learning algorithm we're trying to design!
* In ML you'll hear the marginal probability of y being refered to as the "prior probablity"
* In ML you'll hear the conditional probability of y given x as the "**posterior probability**"
* x is often regarded as feature or object (object with certain features)


![404]({{ site.url }}/images/ml/posterior.PNG)

### Relation to ML
* During the training phase, the training data helps us estimate the "probability distribution over a set of classes" (aka p(label))
* During the testing phase, the training data helps us estimate the "probability that a sample belongs to a class" (aka p(label\|object))

### Conditional probablity for continous variables
![404]({{ site.url }}/images/ml/continuous.PNG)
* If we define feature 1 as having a value between -2 and -1, we can see that p(blue\|feature 1) is 100%

![404]({{ site.url }}/images/ml/class.PNG)

* The training data (historical) eventually estimates the probability distributions for each of the class posterior probabilities
  * Although we start with a "discrete" amount of training data, the computer will estimate for us the continous probability functions based on the data that we have

![404]({{ site.url }}/images/ml/training2.PNG)

* In this example the classes (aple vs orange) are mutually exclusive, therefore the class posterior probabilities add to 1 for any continous value of feature 1

## Decision theory
* Decision theory complements the "soft" classifaction (giving probability of belonging to a class rather than black and white outcomes) with "hard" (black and white) final outcomes.

### Decision boundary
* In order to ultimately classify x to a given class, we *generally* have n-1 decisions boundaries for n classes

![404]({{ site.url }}/images/ml/boundary.PNG)

* The criteria to assign an object to a class is simply that the (posterior) conditional probability of that class is higher than the other ones. Basically the most likely class is selected.

### Soft-Hard classification process recap and complicated decision boundaries
* The training data defines the probability of a feature present in a label (note that this class conditional probability is the **reversed** of the posterior (but it's not the prior, the prior would be \\(p(C_k)\\), we're just reversing the roles of \\(C_k\\) since the training data itself intrinsically describes \\(p(x\|C_k)\\)).
  * (Soft classification) From the training set we do have the marginal probabilities to plug the values into bayes rule and get:
  * $$p(C_k|x)=\frac{p(x|C_k)p(C_k)}{p(x)}$$
  * (Hard classification) Assign the objects to the label that has the highest class posterior probablity \\(p(C_k\|x)\\)

![404]({{ site.url }}/images/ml/complicated.PNG)
![404]({{ site.url }}/images/ml/missing.PNG)
![404]({{ site.url }}/images/ml/2d.PNG)
![404]({{ site.url }}/images/ml/multi.PNG)

### Rewritting the classifier for convience
![404]({{ site.url }}/images/ml/rewrite.png)
* Classifiers are often rewritten i.e. such that the derivative is easier to compute

### Model
* Note on the probability function for the conditional probability \\(p(x\|C_k))\\ (called class conditional probability): The shape of the distribution (utterly based on the training data) is based on the model used in the training phase.
  * We covered a sample [linear moder](#non-probabilstic-example-polynomial-curve-fitting-linear-model) but more details on selecting a model come in next sections
* During training estimate the model parameters such that the example objects fit well: maximum likelihood estimators

### Classification error
![404]({{ site.url }}/images/ml/bayeserror.png)
* This is due to noise in the data as well as how well it can be naturally split into different categories (classifier quality)

![404]({{ site.url }}/images/ml/classification_error.png)
* The yellow part are actually errors (but we can shift the decision boundary to one side to minimize the errors)
* \\(Prob(x \in C_1,x \to C_2)\\) is read as probabilty of x being assigned to c2 but actually being c1.
* The total classification error is basically adding up the yellow spaces, or the weighted sum of errors

![404]({{ site.url }}/images/ml/bayeserror2.PNG)
* Bayes error is the minimum attainable error (not necessarily intersection of 2 distributions but just the place where the yellow area is the smallest)
* In practice we do not have the true distributions, and we cannot obtain them
* The Bayes error does not depend on the classification rule that you apply, but on the distribution of the data
* In general you cannot compute the bayes error
  * You do not know the true class conditional probabilities
  * The high dimensional integrals are very complicated


### Missclassification costs
* We want to make as few errors as possible with the assignment of x to class C
* Error: x is assigned to C1 but should have been assigned to C2 and viceversa
* Sometimes missclassification of class A to class B is much more costly than misclassification of class B to class A
  * We can add a loss value for false positives and false negatives respectively
* Then the goal becomes to minimize the loss function

## Density-based classifiers
* Most basic fundamental assumption for a machine learning algorithm: goal is to estimate the "posterior probability" P(label\|object) (if we already know the real posterior probability, then the job is already done and we just jump onto drawing decision boundaries)
* First we encode the object into a set of features
  * Height
  * Weight
  * Color
  * etc
* The "feature space" has as meany dimensions as there are features
![404]({{ site.url }}/images/ml/feature_space.png)
* The ploted crosses/circles/stars are the training data of this classification task of 3 possible labels (setosa, versicolour, virignica) and 2 features (sepal width and sepal legth)
  * This can be plotted because we have p(x,y) (joint probability of x and y) which is explicitly derived from the training set that has labels
* Then we can draw decision boundaries
![404]({{ site.url }}/images/ml/decision_boundaries.png)
* The underlying criteria to draw this lines is that the posterior probability of an object belonging to that class is higher than to all the other classes (individually)
  * \\(p(y_1\|x) \gt p(y_2\|x)\\)
  * w is also expressed as y to indicate the class
* We can use computers to calculate the probability density functions for each label/class/y/w \\(y_n\\) for a given object x with k features (k denoting the dimension of the model).
  * For the 2 features flowers trainging set we get:
![404]({{ site.url }}/images/ml/model.png)
* For each point in the feature space we should be able to get n posterior probabilities (1 for each class) and all of them should sum up to 1

### Estimating probability (density) functions
* It is very hard to calculate posterior class probability distributions (shape, that they all add up to one, etc)
  * The class conditional distribution is also hard, but that integral has been studied and estimated for much more time and there have been satisfying solutions. (It is thus slightly less hard)
* All estimations are eventually computed from the training set/sample/examples
* Instead of directly estimating the posterior probability we'll use baye's rule to estimate an equivalent distribution based on the baye's rule terms
* $$p(y|x)=\frac{p(x|y)p(y)}{p(x)}$$
  * \\(p(x\|y)\\) is the class conditional distributtion
    * Probability distribution of a feature vector given that it belongs to a certain class: This is equal to p(x,y)/p(y) (which we both know)
    * If we go to the iris flower example, \\(p(x\|\text{iris setosa})\\) is just the integral fuction that wraps approximately all of the blue data points underneath the curve
  * \\(p(y)\\) is the class prior
    * it's a constant (for each discrete class is different), easy to compute (class occurence / total occurences)
  * \\(p(x)\\) is the unconditional data distribution
    * What's the distribution of x regardless of the class
![404]({{ site.url }}/images/ml/data_distribution.png)  
* Since p(x) is a common term in all class comparisions, we can remove it from the inequation, so we just have to compute to terms
* To approximate \\(p(x,y)p(y)\\) which is a poor man's version of \\p(y\|x)\\) (only within the context of comparing classes, as p(x) is defenetly missing in the first one) we will consider:
  * Discriminative and generative models
  * Parametric and nonparametric models

### Histogram-based density estimation
![404]({{ site.url }}/images/ml/histogram.png)
* The problem is to get enough data points to reach the central limit theorem
![404]({{ site.url }}/images/ml/clt.png)
* The thumbrule is to use 1k training objects to the power of the number of features.
* parameter = bin = rectangle
![404]({{ site.url }}/images/ml/bin.png)
* However this is unworkable for probability density functions with more than 2 features as the require sample size increases dramitically with the number of features

### Curse of dimensionality
* There is a trade-off between having more features (and thus distinguishing objects better) and having to estimate density functions that require exponentially more training objects
* The number of parameters (bins) increases with the number of features as well

## Parametric density estimation
* In a parametric model you assume you know the shape of the full distribution (i.e. it should be round, normally distributed, etc.) and the only thing you need to estimate is the paramaters of that well known established shape
* Picking it from where we left it, we're gonna use parametric modeling & estimation for the class conditional probablity (that is, not the class posterior, but \\(p(x\|y)\\)), which we later combine with \\(p(y)\\) to define the classifier: \\(p(x\|y_1)p(y_1) \gt p(x\|y_2)p(y_2)\\)

### Assume Gaussian (Bell shaped) distribution
* By using a known shape we can just focus on estimating it's parameters, for a gaussian that'd be the mean and standard deviation
* However, to make the model accurate, we should choose features that are also distributed in the same way in real life, such as height and weight
![404]({{ site.url }}/images/ml/gauss.png)
* We choose gaussian distribution because it is the one from the central limit theorem
  * sums of large numbers of independent identically distributed random variables will have a gaussian distribution
* It occurs in real life
* It has few paramaters (2 (mean and variance) as opposid as lots of bins for histograms)
  * These parameters are easy to estimate

![404]({{ site.url }}/images/ml/multi_gauss.png)
* Both x and \\(\mu\\) are vectors (aka feature vector and mean feature vecture) and \\(\Sigma\\) is a covariance matrix (variance for higher dimensions)

![404]({{ site.url }}/images/ml/gauss2d.png)
* The red star is the mean vector \\(\mu\\)
* The covariance matrix is k x k with k being the number of dimensions (features)
* On the diagonal we find the variances for each of the features
* \\(\rho_{12}\\) is the correlation coefficient between features 1 and 2

#### Covariance matrix and mean vector in a 2D gaussian distribution
* The mean vector determines the center of the distribution
* The covariance matrix determines the shape in 2 ways
  * The diagonal matrix variances determine the breadth on their respective axis
  * The correlation coefficients determine the "rotation" (it's the "slope" if we watched the data points from the top)

![404]({{ site.url }}/images/ml/gauss_param2d_1.png)
* Note that in the pictures above the mean feature vector is expressed in one column, but in CSE2510 we will express de design matrix in rows for each object. This doesnt change the fact that we'll usea the feature vector as a vector (thus n x 1) (as in the picture)
* Top view of some gauss distributions with 0,0 mean vector
![404]({{ site.url }}/images/ml/gauss_param2d.png)

### Maximum likelihood estimates (for the mean and coveriance matrix)
* Note that the hat denotes estimation variable
* For the mean:
  * $$\hat{\mu}=\frac{1}{n}\sum^{n}_{i=1}x_i$$
  * just sum all feature vectors and divide all entries by the number of objects in the data set
* For the covariance matrix:
  * $$\hat{\Sigma}=\frac{1}{n}\sum^{n}_{i=1}((x_i-\hat{\mu})(x_i-\hat{\mu})^T)$$
  * T denotes the transpose
  * Multipling an n x 1 matrix with it's transpose makes a n x n matrix
* They are estimations because they are based on the training data and may not be exactly the same as the real parameters
* If you recall the gaussian model for p-dimensions, we need the inverse of the covariance matrix.
  * The covariance matrix will be inversible only with at least \\(0.5p(p+1)\\) data points
  * The number of data needed increases quadratically
  * For a 32 by 32 pixels images you need at least \\(32^2\\) examples per class

![404]({{ site.url }}/images/ml/noinv.png)

## Gausian density based classifiers
* As discussed earlier, the classifier is to classify to class 1 when \\(p(y_1\|x) \gt p(y_2\|x)\\) or the proportionally equivalent inequation \\(p(x\|y_1)p(y_1) \gt p(x\|y)p(y_2)\\)
* The model behind either posterior or class conditional density distribution is the gauss bell shape distribution
  * For each class y we have a gaussian distribution
  * ![404]({{ site.url }}/images/ml/class_gaussian.png)
  * We have to estimate the parameters \\(\hat{\mu}\\) and \\(\hat{\Sigma}\\)
* recall that all density functions derived from the training set are only estimations of the real world distributions, and hence the "hat"

![404]({{ site.url }}/images/ml/2distros.png)

### Two-Class case
* We can use the log of the conditionals as the classifier.
* Instead of using an inequality, we just do the subtraction and if the number is positive it goes to the class on the left operand class and if it's negative it goes to the right operand class.
* The function is called the "discriminant" and it's a quadratic classifier because the decision boundary is a quadratic function of x

![404]({{ site.url }}/images/ml/log.png)

* Taking the logs encapsulates the gaussian distribution inside a log, whose exponent is cancelled out, (the other term of the product is a constant that we can ignore) and we're left with a quadratic distribution in terms of the feature vector x

![404]({{ site.url }}/images/ml/log2.png)
![404]({{ site.url }}/images/ml/log3.png)
![404]({{ site.url }}/images/ml/log4.png)

* The black line of the quadratic classifier below is the set of x features that have a 0 output from the discriminant function (neither one class nor the other, the decision boundary)

![404]({{ site.url }}/images/ml/quadratic_classifier.png)

* The quadratic (hiperbola) discrminant function can take multiple shapes (within quadratic nature) depending on the shapes of the distributions of the classes (depending on the covariances)

![404]({{ site.url }}/images/ml/linear.png)

* Linear when the covariances are the same (and the means are different)

![404]({{ site.url }}/images/ml/circular.png)

* Happens when one of the classes is spread across the feature space while the other class is concentrated in a smaller area

![404]({{ site.url }}/images/ml/hyperbolic.png)

* 2 decision boundaries described by a single function

### Fixing no inverse covariance matrix for Two-classes case
* Assume that gaussian shape of the other class is the same (i.e. just differeant mean but same covariance)
![404]({{ site.url }}/images/ml/noinv2.png)
  * This allows us to use all our data from all classes to estimate 1 single covariance matrix
  * The 1/C times sum of C elements is just the average of the term inside the summation
* This makes the classifier a linear function since the W term is multiplied by 0 by assuming that all class covariances are the same
  * We are then just left with the constant \\(w_o\\) and with \\(w^Tx\\)

![404]({{ site.url }}/images/ml/log4.png)

* We end up with the LDA (linear discriminant analysis), assumes both distributions to have the same covariance matrix

![404]({{ site.url }}/images/ml/lda.png)

* It's the classic (now rather old) approach
* It's simple and fast
* Doesn't need enough data
* Works well for simple data sets
* Never the best, but always in the top best when multiple features are used

### Linear discriminant
![404]({{ site.url }}/images/ml/ld.png)
* The way w was defined makes it a vector that is perpendicular to the decision boundary
* \\(w_0\\) is a constant that shifts the decision boundary
  * A positive constant shifts the decision boundary to the left, and a negative one to the right

![404]({{ site.url }}/images/ml/wTx.png)

* Recall that w and \\(w_0\\) can be both found just by having the mean and variance parameters of the distribution

![404]({{ site.url }}/images/ml/ld_classifier.png)

* This dot product of w and x allows us to observe that certain "weights" (entries) of w have larger impact on the dot product outcome (by being larger), thus we can observe from w which features are important for the classification.
  * Those equal or close to zero could be removed

#### Rewriting the bias term (\\(w_0\\))
![404]({{ site.url }}/images/ml/bias.png)
* Sometimes called homogenous coordinates

## Nearest Mean Classifier (NMC)
No estimated covariance matrix:
* When the number of features far exceeds the sample size it becomes hard to estimate even the average covariance matrix
* Instead one can assume that all features have the same variance and all are uncorrelated
* Thus the covariance matrix is justa diagonal matrix with the same values, the variance of all features:
![404]({{ site.url }}/images/ml/no_cov.png)
![404]({{ site.url }}/images/ml/nearest_mean.png)
![404]({{ site.url }}/images/ml/nmc.png)

### Feature scaling
* When a classifier depends on euclidiean distances, scaling of the features matter
* Changing the scaling can improve/deteriorate the classifier (check the previous NMC example with different scales)
* A good practice is to standardize the features (objects) to Z scores (how many standard deviations away is a feature from the mean), here they are denoted as 
  * $$\tilde{x} = \frac{x - \mu}{\sigma}$$

## Model complexity and available data trade-off
* Using plug-in Bayes' rule with normal distribution for every class can give rise to different classifiers. From more flexible/complex to more simple:
  * **Quadratic classifier**: Separate mean and covariance matrix per class
  * **Linear classifier**: Separate mean, equal covariance matrix per class
  * **Nearest mean classifier**: Separate mean, same diagonal covariance matrix per class
    * This is the only one from the list that suffers from non-standardize data points
* More flexible classifier needs more training data
* Simple classifiers still perform well in practice
* **Curse of dimensionality**: The more features the more training data required

## Lab: univariate classifier in Pyton

