---
layout: post
title:  "Big Data Processing summary"
date:   2021-09-01 00:51:00 +0200
categories: datascience
tags: CSE2520
---
{% include math.html %}
<!--more-->

This is a summary of CSE2520 Big Data Processing

# Table of Contents
- [Table of Contents](#table-of-contents)
  - [Big and Fast Data](#big-and-fast-data)
    - [What is big data?](#what-is-big-data)
    - [How big is "big"?](#how-big-is-big)
    - [Vs of Big data:](#vs-of-big-data)
      - [Volume](#volume)
      - [Variety](#variety)
      - [Velocity](#velocity)
  - [Big data processing](#big-data-processing)
    - [ETL cycle](#etl-cycle)
    - [Big data engenieering](#big-data-engenieering)
    - [Big data analytics](#big-data-analytics)
    - [Batch processing](#batch-processing)
    - [Stream processing](#stream-processing)
    - [Data processing distribution](#data-processing-distribution)
    - [Desired properties of a big data processing system](#desired-properties-of-a-big-data-processing-system)
    - [Large-scale computing](#large-scale-computing)
    - [Big Data tech timeline](#big-data-tech-timeline)
    - [Current Big Data tech landscape](#current-big-data-tech-landscape)
  - [Problems solved with Big Data](#problems-solved-with-big-data)
  - [Big Data Programming Languages: Scala & Python](#big-data-programming-languages-scala--python)
    - [Hello world](#hello-world)
    - [Declarations](#declarations)
    - [Declaring functions](#declaring-functions)
    - [Declaring classes](#declaring-classes)
    - [Inheritance](#inheritance)
    - [Data classes](#data-classes)
    - [Pattern matching in Scala](#pattern-matching-in-scala)
    - [Basic data types](#basic-data-types)
      - [Types of data](#types-of-data)
      - [Sequences/Lists](#sequenceslists)
      - [Sets](#sets)
      - [Maps or Dictionaries](#maps-or-dictionaries)
      - [Graphs](#graphs)
      - [Nested data types: Trees](#nested-data-types-trees)
      - [Tuples](#tuples)
      - [Relations](#relations)
      - [Key/Value pairs](#keyvalue-pairs)
  - [Functional programming](#functional-programming)
    - [Function signatures](#function-signatures)
    - [Side effects](#side-effects)
    - [Pure functions](#pure-functions)
    - [Immutable data structures](#immutable-data-structures)
      - [Pattern matching on data structures](#pattern-matching-on-data-structures)
    - [Higher-order functions](#higher-order-functions)
      - [Apply to all](#apply-to-all)
      - [Important higher-order functions](#important-higher-order-functions)
      - [Aux higher-order functions](#aux-higher-order-functions)
    - [Laziness](#laziness)
    - [Monads](#monads)
    - [Enumerating datasets](#enumerating-datasets)
      - [Iteration](#iteration)
      - [Observation](#observation)
      - [Traversal](#traversal)
    - [Operations](#operations)
      - [Element-wise operations](#element-wise-operations)
      - [Aggregations](#aggregations)
        - [Grouping](#grouping)
    - [Key value pairs](#key-value-pairs)
      - [Common operations](#common-operations)
      - [KV pair to relation and viceversa](#kv-pair-to-relation-and-viceversa)
  - [Big data Immutability](#big-data-immutability)
    - [Copy-on-write (COW)](#copy-on-write-cow)
    - [Immutable/persistent datastructures](#immutablepersistent-datastructures)
    - [Data structures in Scala](#data-structures-in-scala)
  - [Why use Scala?](#why-use-scala)
  - [Distributed Systems](#distributed-systems)
    - [Characteristics](#characteristics)
    - [Fallacies](#fallacies)
    - [Problems](#problems)
    - [Asynchronous vs syncrhonous systems](#asynchronous-vs-syncrhonous-systems)
    - [Time is essential](#time-is-essential)
      - [Logical time](#logical-time)

## Big and Fast Data
### What is big data?
* Besides a buzzword is also used to describe:
  * *Data too large to be efficiently processed on a **single computer***
  * *Massive amounts of **diverse**, **unstructured** data produced by **high-performance applications***

### How big is "big"?
* Typical numbers associated with big data:
  * 2.5 Exabytes (\\(2.5 \cdot 10^6 TB\\)) produced daily
  * IoT: 21.5 billion devices with internet access
  * Facebook, Amazon, Microsoft and Google store at least 1,200 petabytes of information
  * 100k google seraches per second
    * each query involves more than 1k machines
    * each query search touches more than 200 services
  * Amazon processes more than 1k orders per second
  * 1 billion of daily instagram users

### Vs of Big data:
Main Vs:

* Volume
* Variety (different forms and sources)
* Velocity (content changes quickly)

Other:

* (Business) value
* Veracity (accuracy)
* Validity (interpretation)
* Visibility
* Volability
* Virality

#### Volume
* We call big data big because of the volume
  * 90% of all data ever was created in the last 2 years
  * The global big data and business analytics market was valued at 138.9 billion in 2020 and is expected to grow

![404]({{ site.url }}/images/bd/growth.png)

#### Variety
* Structured data: SQL tables, images, format is known
* Semi-structured data: JSON, XML
* Unstructured data: Text

#### Velocity
* Big data is not just big (volume) (and varied), but it's also generated and processed fast:
  * Data centers write a lot to log files
  * Social media posts
  * Stock market high-frequency trading (latency costs money)
  * Online advertising
* Data needs to be processed with soft or hard real-time guarantees

## Big data processing
### ETL cycle
* Extract: Convert raw or semi-structured data into structured data (i.e. JSON to database tables)
* Transform: Convert units, join data sources, clean data...
* Load: load the data into another system for further processing

### Big data engenieering
* It's about building "pipelines"

### Big data analytics
* It's about discovering patterns

### Batch processing
* All data exists in some data store, a program processes the whole dataset at once (i.e. FRISS csv historical fraud batches)

### Stream processing
* Processing of data as they arrive to the system (i.e. FRISS real time fraud score)

### Data processing distribution
* Divide the data (i.e. csv of historical fraud) in chunks and apply the same task on all chunks at the same time, i.e. via multiple machines/CPUs with each machine assigned with it's unique chunk (data-parallelism: one task, many data splits)
* If possible, divide the task into independent sub-tasks that use the same data source (i.e. replace(",",".") for all records in a column and at the same time replace blanks with "0.0")

### Desired properties of a big data processing system
* Robustness and fault-tolerance
* Low latency reads and updates
* Scalability
* Generalization
* Extensibility
* Ad hoc queries
* Minimal maintenance
* Debuggability

### Large-scale computing
* Emerged in the 70's
* Phisicists used super computers for simulations in the 80's
* Shared-memory designs are still in large scale use
* What's new is: Large scale processing on **distributed**, **commodity** computers (i.e. average linux user home computer) enabled by advanced software using **elastic** resource allocation
* It is **software** and not hardware what drives the Big Data industry

### Big Data tech timeline
Progress is mostly industry-driven:

* 2003: Google publishes the Google Filesystem paper, a large-scale distributed file system
* 2004: Google publishes the Map/Reduce paper, a distributed data processing abstraction
* 2006: Yahoo creates and open sources Hadoop, inspired by the Google papers
* 2006: Amazon launches its Elastic Compute Cloud, offering cheap, elastic resources
* 2007: Amazon publishes the DynamoDB paper, sketches the blueprints of a cloud-native database
* 2009 – onwards: The NoSQL movement. Schema-less, distributed databases defy the SQL way of storing data
* 2010: Matei Zaharia et al. publish the Spark paper, brings FP to in-memory computations
* 2012: Both Spark Streaming and Apache Flink appear, able to handle really high volume stream processing
* 2012: Alex Krizhevsky et al. publish their deep learning image classification paper re-igniting interest in neural networks and solidifying the value of big data


### Current Big Data tech landscape
![404]({{ site.url }}/images/bd/landscape.png)

## Problems solved with Big Data
* Modelling: What factors influence particular outcomes/behaviour?
* Inormation retrivals: Search engines, web scrappers
* Collaborative filtering: Recommending items based on items other users with similar tests ahve chosen
* Outlier detection: Discovering outstanding transactions

## Big Data Programming Languages: Scala & Python
* The Big data and data science languages are
  * **scala**: for intensive systems
    * Strong point is the combination of functional programming and object oriented programming
    * [documentation](https://docs.scala-lang.org/)
  * **python**: for data analytics tasks
    * Strong point is the combination of object oriented and imperative programming
    * [documentation](https://docs.python.org/2/)
  * Both support object oriented programming, functional programming and imperative programming. But python is interpreted and scala is combined
* Other languages include
  * **Java**: the language in which most big data infrastructure is written into
  * **R**: Statistics language with great selection of libraries for serious data analytics and plotting tools

* **In CSE2520 we will be using only scala**


### Hello world
* Scala is not only similar to java, but it can also actually run java code itself
  * Both Scala and Java are compiled to JVM bytecode
  * Scala can interoperate with JVM libraries
  * Scala is not sensitive to spaces/tabs. Blocks are denoted by `{}`

```scala
object Hello extends App {
    println("Hello, world")
    for (i <- 1 to 10) {
      System.out.println("Hello")
    }
}
```

* Hello world in Python
  * Python is interpreted
  * Python is indentation sensitive: blocks are denoted by a TAB or 2 spaces.

```py
for i in range(1, 10):
  print("Hello, world")
```

### Declarations
* Scala:
  * Type inference used extensively (no need to explicitly declare the type of the variable like in js, but you may do it)
  * Two types of variables: vals are constants, vars are variables
  * In CSE2520 we will only use `val`

```scala
object Declarations extends App {
  var a: Int = 5
  val b = 6

  println(a)

  //b = 6 // wont compile program because val is like js' "const"

  println(b)

  // Type of foo is inferred
  val foo = new Array[Int](5)

  // var a = "Foo" // wont compile because a is already defined with val
  var c = "Foo"
  // c = 42 // won't compile due to type mismatch
  c = "Bar"

  print(c)
}
```

* python:
  * Optional typing, not enforced at runtime

```py
import numpy as np

a: int = 5
a = "Foo"

a = np.array([a, 6, 7, 8])

print(a)
```
`['Foo' '6' '7' '8']`

### Declaring functions
* Scala:
  * Statically typed
  * Evaluated expressions have types
  * The return type is the most generic type of all return expressions

```scala
object Functions extends App {
 println(max(3,1))

  def max(x: Int, y: Int): Int =
    if (x >= y) x else y
}
```

* Python:
  * Dynamically typed
  * Mostly based on statements
  * Types are optional


```py
def maxi(x: int, y: int) -> int:
    if x >= y:
        return x
    else:
        return y


print(maxi(5, 3))  # you cant use functions before they're defined since python is executed line by line
```

### Declaring classes
* Scala
  * A default constructor is created automatically

```scala
class ClassExample(
                    val x: Int,
                    var y: Double = 0.0
                  )
```
```scala
  // Type of a is inferred
  val a = new ClassExample(1, 4.0)
  println(a.x) //x is read-only
  println(a.y) //y is read-write
  a.y = 10.0
  println(a.y) //y is read-write
```
```
1
4.0
10.0
```

* Python

```py
class Foo:
    def __init__(self, x, y):
        self.x = x
        self.y = y
```
```py
import class_example as ce

a = ce.Foo(3, 2)
print(a.x)

a.x = "foo"
print(a.x)

```
```
3
foo
```

### Inheritance
* Scala
  * Traits are equivalent to java interfaces (abstract classes (cant be initiliazed itself) whose methods don't have body) and includes attributes

```scala
object Inheritance extends App {
  var c = new Baz(5, 6f, 7)
  println(c.asString())
}


class Foo(val x: Int,
          var y: Double = 0.0)

class Bar(x: Int, y: Int, z: Int)
  extends Foo(x, y)

trait Printable {
  val s: String

  def asString(): String
}

class Baz(x: Int, y: Double, private val z: Int)
  extends Foo(x, y) with Printable {
  override val s: String = new String( //java code
    String.valueOf(x)
      + " "
      + String.valueOf(y)
      + " "
      + String.valueOf(z))

  override def asString(): String = s
}
```
`5 6.0 7`

* Python

```py
class TwoDimensionPoint:
    def __init__(self, x, y):
        self.x = x
        self.y = y


class ThreeDimensionPoint(TwoDimensionPoint):
    def __init__(self, x, y, z):
        TwoDimensionPoint.__init__(self, x, y)
        self.z = z


a = TwoDimensionPoint(1, 2)
b = ThreeDimensionPoint(10, 20, 30)

print(a.x, a.y)
print(b.x, b.y, b.z)
```
```
1 2
10 20 30
```

* In both scala and python children can override parents

### Data classes
* Data classes are blueprints for immutable objects.
* We use them to represent data records.
* Both languages implement equals (or __eq__) for them, so we can compare objects directly.

* Scala:

```scala
object DataClass extends App {
  val p = Person("Name", Address("Street", 2))
  // p.name = "sergio" //wont compile

  println(new String(p.name + " lives at " + p.address.street + " " + p.address.number))
}

case class Address(street: String,
                   number: Int)

case class Person(name: String,
                  address: Address)
```
`Name lives at Street 2`

* Python:

```py
from dataclasses import dataclass


@dataclass
class Address:
    street: str
    number: int


@dataclass
class Person:
    name: str
    address: Address


p = Person("G", Address("a", 2))
p.name = "Sergio"  # does compile
print(p)
```
`Person(name='Sergio', address=Address(street='a', number=2))`

### Pattern matching in Scala
* It's the equivalent of switch in java

* Java:

```java
public String getInstruction() {
        switch (instruction) {
            case LDA:
                return "0001";
            case ADD:
                return "0010";
            case SUB:
                return "0011";
            case STA:
                return "0100";
            case LDI:
                return "0101";
            case JMP:
                return "0110";
            case JPC:
                return "0111";
            case JPZ:
                return "1000";
            case OUT:
                return "1110";
            case HLT:
                return "1111";
            default:
                return "0000";
        }
    }
```

* Scala:

```scala
object PatternMatching extends App {
  val instruction = "LDA"

  def getInstruction(opcode: String): String =
    instruction match {
      case "LDA" => "0001"
      case "ADD" => "0010"
      case "SUB" => "0011"
      case "STA" => "0100"
      case "LDI" => "0101"
      case "JMP" => "0110"
      case "JPC" => "0111"
      case "JPZ" => "1000"
      case "OUT" => "1110"
      case "HLT" => "1111"
      case _ => "0000"
    }

  println(getInstruction(instruction))
  
}
```
`0001`

### Basic data types
#### Types of data
* Unstructured: Data whose format is not known
  * Raw text documents
  * HTML pages
* Semi-structured: Data with a known format
  * Pre-parsed data to standard formats: JSON, CSV, XML
* Structured: Data with known formats, linked together in graphs or tables
  * SQL or graph databases
  * Images

#### Sequences/Lists
* Basic properties:
  * Size is bounded by memory
  * Items can be accessed by an index (`list1[i]` or `l[j]`)
  * Items can only be inserted at the end (append)
  * Can be sorted

* Python

```py
list1 = [1, 2, 3, 4]
```

* Although numpy arrays are actually more handy

```py
import numpy as np

list1 = [1,2,3,4]
array1 = np.array(list1)
```

* Scala

```scala
val l = List(1, 2, 3, 4)
```

#### Sets
* Stores unique value without any particular order
  * Size is bounded by memory
  * Can be queried for containment
  * Set operations: union, intersection, difference, subset
* Scala:

```scala
val s = Set(1, 2, 3, 4, 4)
s: scala.collection.immutable.Set[Int] = Set(1, 2, 3, 4)
```

#### Maps or Dictionaries
* Maps or Dictionaries or Associative arrays are a collection of `(k,v)` pairs in such a way that each `k` appears only once.
  * Accessomg a value given a key is very fast (\\(O(1)\\))

* Python

```py
values = {

     "key1" : 1.0,

     "key2" : 2,

     "key3" : [1,2,3]

 }
 ```

 * Scala

```scala
val m = Map(("a", 1), ("b",2))
val m: scala.collection.immutable.Map[String,Int] = Map(a -> 1, b -> 2)
```

#### Graphs
* A graph data structure consists of a finite set of vertices or nodes
  * if these nodes/vertices are stored in ordered pairs, the graph is "directed"
  * If the nodes/vertices are stored in unordered pairs, then it's an undirected graph
* Nodes can contain attributes
* Edges can contain weights and directions
* Graphs are usually represented as `Map[Node, List[Edge]]` where in Scala:

```scala
case class Node(id: Int, attributes: Map[A, B])
case class Edge(a: Node, b: Node, directed: Option[Boolean],
                  weight: Option[Double] )
```

#### Nested data types: Trees
* They are ordered graphs without loops
  * Which is basically a set of nested maps

```json
a = {"id": "5542101946", "type": "PushEvent",
    "actor": {
      "id": 801183,
      "login": "tvansteenburgh"
    },
    "repo": {
      "id": 42362423,
      "name": "juju-solutions/review-queue"
   }
}
```

* If we parse the above JSON in almost any language, we get a series of nested maps. In Scala:

```scala
Map("id" -> 5542101946L,
    "type" -> "PushEvent",
    "actor" -> Map("id" -> 801183.0, "login" -> "tvansteenburgh"),
    "repo" -> Map("id" -> 4.2362423E7, "name" -> "juju-solutions/review-queue"))
)
```

#### Tuples
* An n-tuple is a sequence of n elements, whose types are known.
* Scala

```scala
val record = Tuple4[Int, String, String, Int] (1, "Matt", "Damon", 1970)
// alternatively
val record = (1, "Matt", "Damon", 1970)
```

* You can also have nested tuples (recall that scala automatically infers the types of the tup0le contents, including another tuple)

```scala
val a = (1, ("Foo", 2)) // type: Tuple2[Int, Tuple2[String, Int]]
// alternatively: (Int, (String, Int))

println(a._1) // prints 1
println(a._2._1) // prints Foo
```

#### Relations
* A relation is a Set of n-tuples (d1,d2,...,dn) of the same type; one of the tuple elements denotes a key. Keys cannot be repeated.
* Relations are very important for data processing, as they form the theoretical framework (Relational Algebra) for relational (SQL) databases.
* Typical operations on relations are insert, remove and join. Join allows us to compute new relations by joining existing ones on common fields.

* Scala

```scala
val movie1 = (1, "Martian", "PG-13", 2015, 2)
val movie2 = (2, "Prometheus", "R", 2012, 2)
val movie3 = (3, "2001: Space Odyssey", "G", 1968, 1)

val movies = Set(movie1, movie2, movie3)

val stanley = (1, "Stanley Kubrick", 1928)
val ridley = (2, "Ridley Scott", 1937)

val directors = Set(stanley, ridley)
```

#### Key/Value pairs
* A key/value pair (or K/V) is a more general type of a relation, where each key can appear more than once.

* Scala

```scala
// We assume that the first Tuple element represents the key
val a = (1, ("Martian", 2015))
val b = (1, ("Prometheus", 2012))

val kv = List(a, b)
// type: List[(Int, (String, Int))]
```

* Another way to represent K/V pairs is with a Map

```scala
val xs = Map(1 -> List(("Martian", 2015), ("Prometheus", 2012)))
// type: Map[Int, List[(String, Int)]]
```

* K and V are flexible: that’s why the Key/Value abstraction is key to NoSQL databases, including MongoDB, DynamoDB, Redis etc.

## Functional programming
* The basics of functional programming apply to data processing with tools like Hadoop, Spark and Flink.
* Functional programming is a programming paradigm where programs are constructed by applying and composing functions.
* Functional programming characteristics:
  * Absence of side-effects: A function, given an argument, always returns the same results irrespective of and without modifying its environment.
  * Immutable data structures: Side-effect free functions operate on immutable data.
  * Higher-order functions: Functions can take functions as arguments to parametrize their behavior
  * Laziness: The art of waiting to compute till you can wait no more
* Functional programming comes from lambda calculus, a formal mathematical logic system for expressing computation based on functions that operate on immutable data

### Function signatures

$$foo(x:[A], y:B)\to C$$

* Function foo takes as arguments an array/list of type A and an argument of type B and returns an argument of type C
  * foo = function name
  * x and y = names of function arguments
  * \[A\] and B = types of function arguments
  * \\(\to\\) = denotes return type
  * C = type of the returned result
  * \[A\] = denotes that type A can be traversed (i.e. an array)

### Side effects
* A function has a side effect if it modifies some state outside its scope or has an observable interaction with its calling functions or the outside world besides returning a value.

```py
var max = -1

def greaterOrEqual(a: Int, b: Int): Boolean = {
  if(a >= b) {
      max = a // side effect!
      true
    } else {
      max = b // side effect!
      false
    }
}
```

* As a general rule, any function that returns nothing (`void` or `Unit`) does a side effect!
* Example of side effects
  * Modifying a variable
  * Modifying a data structure in place: In FP, data structures are always persistent.
  * Setting a field on an object: OO is not FP!
  * Throwing an exception or halting with an error: In FP, we use types that encapsulate and propagate erroneous behavior
  * Printing to the console or reading user input, reading writing to files or the screen: In FP, we encapsulate external resources into Monads.

### Pure functions
* A pure function depends only on its declared inputs and its internal algorithm to produce its output.
  * It does not read any other values
  * It does not have any side effects

```py
def greaterOrEqual(a: Int, b: Int, max: Int): (Boolean, Int) = {
  if(a >= b) (true, a)
  else (false, b)
}
```

* Pure functions offer referential transparency.
  * An expression is said to be referentially transparent if it can be replaced by its value and not change the program’s behavior.
  * Referential transparency enables simpler reasoning about programs.

### Immutable data structures
* Functional data structures are operated on pure functions and they are immutable
* Scala has immutable lists, tuples, maps and sets

```scala
val oneTwoThree = List(1, 2, 3)
val oneTwoThree_2 = 1 :: 2 :: 3 :: Nil
val one = oneTwoThree.head
val twoThree = oneTwoThree.tail
```

* Scala has both mutable and immutable versions of many common data structures. If in doubt, use immutable.

#### Pattern matching on data structures
* In the context of functional programming pattern matching is just the java equivalent of switch
  * Syntax sugar for if else
* In scala, on top of checking if the contents of the object equals the content of the "switch" case, scala's "switch" feature also allows you to check for type matching and to match for certain values in a list:

```scala
object PatternMatching extends App {

  val x = List(1, 2, 3, 4, 5) match {
    case x :: 2 :: 4 :: xs => x
    case Nil => 42
    case x :: z :: 3 :: 4 :: xs => x + z
    case h :: t => h
    case _ => 404
  }
  println(x)
}
```
`3`

* Note that for a list in scala in the pattern matching using a variable nambe for one of the element is like using a wildcard, which we can use to extract the value from the list at the position the wildcard was used

### Higher-order functions
* A higher-order function is a function that can take a function as an argument or return a function

```scala
// Apply f to all elements of list
def filter(xs: List[A], f: A => Boolean) : List[A]
```
#### Apply to all
* Using applyToAll to define other functions:

```scala
def incrementAll2(ints: List[Int]): List[Int] = applyToAll(ints, (x:Int) => x + 1)
def doubleAll2(ints: List[Int]): List[Int] = applyToAll(ints, (x:Int) => x * 2)
```

* See how `applyToAll(ints, (x:Int) => x + 1)` is like a foreach loop
  * For each `int x` in `ints` return `x + 1`

* But besides functional programming being a set of syntactic sugar to define functions in one line, it's also a philosophy of coding (i.e. "no side effects", "functions are first class citizens", "immutable data structures" (no states), "laziness")

* Scala collections has built-in `map` function to abstract the syntactic sugar even more:

```scala
def incrementAll3(ints: List[Int]): List[Int] = ints.map((x:Int) => x + 1)
def doubleAll3(ints: List[Int]): List[Int] = ints.map((x:Int) => x * 2)
```

#### Important higher-order functions
* `map(xs: List[A], f: A => B) : List[B]`
  * Applies `f` to all elements and returns a new list.
* `flatMap(xs: List[A], f: A => List[B]) : List[B]`
  * Like map, but flattens the result to a single list.
* `foldL(xs: List[A], f: (B, A) => B, init: B) : B`
  * Takes `f` of 2 arguments and an init value and combines the elements by applying `f` on the result of each previous application. AKA `reduce`.

#### Aux higher-order functions
* `groupBy(xs: List[A], f: A => K): Map[K, List[A]]`
  * Partitions `xs` into a map of traversable collections according to a discriminator function.
* `filter(xs: List[A], f: A => Boolean) : List[A]`
  * Takes a predicate and returns all elements that satisfy it
* `scanL(xs: List[A], f: (B, A) => B, init: B) : List[B]`
  * Like `foldL`, but returns a list of all intermediate results
* `zip(xs: List[A], ys: List[B]): List[(A,B)]`
  * Returns an iterable collection formed by iterating over the corresponding items of xs and ys.

### Laziness
* Laziness is an evaluation strategy which delays the evaluation of an expression until its value is needed
  * Separating a pipeline construction from its evaluation
  * Not requiring to read datasets in memory: we can process them in lazy-loaded batches
  * Generating infinite collections
  * Optimizing execution plans

```scala
// Scala LazyList
val fibs: LazyList[Int] = {
  0 #:: 1 #:: fibs.zip(fibs.tail).map{ n =>
    println("Adding " + n._1 + " and " + n._2)
    n._1 + n._2
  }
}
fibs.take(5).foreach(println)
```
```
## 0
## 1
## Adding 0 and 1
## 1
## Adding 1 and 1
## 2
## Adding 1 and 2
## 3
```

### Monads
* Monads are the tool FP uses to deal with (side-)effects
  * a design pattern that defines how functions can be used together to build generic types.
  * s a value-wrapping type that:
    * Has an identity function
    * Has a flatMap function, that allows data to be transferred between monad types
* Example monads:
  * Null points: `Option[T]`
  * Exceptions: `Try[T]`
    * `Success[T]`, where T represents the type of the result
    * `Failure[E]`, where E represents the type of error, usually an exception
  * Latency in asynchronous actions: `Future[T]`

* In functional programming exceptions are preferably not used as they break referential transparency. The solution is to return a predefined bogus value
  * Allows errors to silently propagate
  * Not applicable to polymorphic code
  * Difficult to use the result – requires special policy

### Enumerating datasets
* Before starting to process datasets, we need to be able to go over their contents in a systematic way. The process of visiting all items in a dataset is called traversal.
* In a big data system:
  * Client code processes data
  * A data source is a container of data (e.g. array, database, web service)
* There are two fundamental techniques for the client to process all available data in the data source
  * Iteration: The client asks the data source whether there are items left and then pulls the next item.
  * Observation: The data source pushes the next available item to a client end point.

#### Iteration
* iteration allows us to process finite-sized data sets without loading them in memory at once.

```scala
trait Iterator[A] {
  def hasNext: Boolean
  def next(): A
}
```

* Typical usage

```scala
val it = Array(1,2,3,4).iterator
while(it.hasNext) {
  val i = it.next + 1
  println(i)
}
```

* The Iterator pattern is supported in all programming languages.

```scala
val data = scala.io.Source.fromFile("/big/data").getLines
while (data.hasNext) {
  println(data.next)
}
// Equivalently...
for (line <- data) {
  println(line)
}
```

#### Observation
* Observation allows us to process (almost) unbounded size data sets, where the data source controls the processing rate

```scala
// Consumer
trait Observer[A] {
  def onNext(a: A): Unit
  def onError(t: Throwable): Unit
  def onComplete(): Unit
}

// Producer
trait Observable[A] {
  def subscribe(obs: Observer[A]): Unit
}
```

* Typical usage

```scala
Observable.from(1,2,3,4,5).
  map(x => x + 1).
  subscribe(x => println(x))
```

#### Traversal
* We apply a strategy to visit all individual items in a collection.

* Python example

```py
for i in [1,2,3]:
  print i

for k,v in {"x": 1, "y": 2}:
  print k
```

* In case of nested data types (e.g. trees/graphs), we need to decide how to traverse. Common strategies include:
  * Breadth-first traversal: From any node A, visit its neighbors first, then its children.
  * Depth-first traversal: From any node A, visit its children first, then its neighbors.
* In most programming environments, traversal is implemented by iterators.

### Operations
* Operations are transformations, aggregations or cross-referenceing of data stored in data types. All of container data types can be iterated.
* We generally have two types of operations:
  * Element-wise ops apply a function to each individual message. This is the equivalent of map or flatMap in batch systems.
  * Aggregations group multiple events together and apply a reduction (e.g. fold or max) on them.

#### Element-wise operations
* Conversion: Convert values of type A to type B
  * This is generalized to the map function:
    * Celcius to Kelvin
    * € to $
* Filtering: Only present data items that match a condition
  * All adults from a list of people
  * Remove duplicates
* Projection: Only present parts of each data item
  * From a list of cars, only display their brand

#### Aggregations
* Aggregations apply a combining operator on a traversable sequence to aggregate the individual items into a single result.
* Aggregation is implemented using reduction (or folding). Two variants exist
  * With left reduction, we traverse items from the first to last
  * With right reduction, we traverse items from the last to first
  * The end result of reduceR abd reduceL is the same iff the operation is commutative:
    * An operation `o` is commutative if x `o` y = y `o` x
* Example of aggregation function: calculate the total sum of a list of integers

##### Grouping
* Grouping splits a sequence of items to groups given a classification function: \\(groupBy(xs:[A], f: A \rightarrow K): Map[K, [A]]\\)

```python
def group_by(classifier, xs):
  result = dict()
  for x in xs:
    k = classifier(x)
    if k in result.keys():
        result[k].append(x)
    else:
        result[k] = [x]
  return result
```
```py
def number_classifier(x):
  if x % 2 == 0:
    return "even"
  else:
    return "odd"

a = [1,2,3,4,5,6,7]
print(group_by(number_classifier, a))
```
`## {'odd': [1, 3, 5, 7], 'even': [2, 4, 6]}`

### Key value pairs
* The most common data structure in big data processing is key-value pairs.
  * A key is something that identifies a data record.
  * A value is the data record. Can be a complex data structure.
  * The KV pairs are usually represented as sequences
* KV stores is the most common format for distributed databases
  * What KV systems enable us to do effectively is processing data locally (e.g. by key) before re-distributing them for further processing. Keys are naturally used to aggregate data before distribution. They also enable (distributed) data joins.
  * Typical examples of distributed KV stores are Dynamo, MongoDB and Cassandra

* Python

```py
[ # Python
  ['EWI': ["Mekelweg", 4]],
  ['TPM': ["Jafaalaan", 5]],
  ['AE': ["Kluyverweg", 1]]
]
```
```scala
List( // Scala
  List("EWI", Tuple2("Mekelweg", 4)),
  List("TPM", Tuple2("Jafaalaan", 5)),
  List("AE",  Tuple2("Kluyverweg", 1))
)
```

#### Common operations
Recall that in these scenarios the keys do no not need to be unique:
* `mapValues`: Transform the values part:
\\(mapVal(kv: [(K,V)], f: V\rightarrow U): [(K,U)]\\)
* `groupByKey`: Group the values for each key into a single sequence.
\\(groupByKey(kv: [(K,V)]) : [(K, [V])]\\)
* `reduceByKey`: Combine all elements mapped by the same key into one
\\(reduceByKey(kv: [(K,V)], f: (V,V) \rightarrow V) : [(K, V)]\\)
* `join`: Return a sequence containing all pairs of elements with matching keys
\\(join(kv1: [(K,V)], kv2: [(K,W)]) : [(K, (V,W))]\\)

* In functional programming we desire to apply transformations to a separate new instances and keep the original data untouched

#### KV pair to relation and viceversa
* A KV pair is an alternative form of a relation, indexed by a key. We can always convert between the two

* Relation to KV pair

```scala
val kvpair : Set[Tuple2[Int, Tuple2[Int, String]]] =
  relation.map(x => (x._1, (x._2, x._3)))
```

* KV pair to relation

```scala
val relation2: Set[Tuple3[Int, Int, String]] =
  kvpair.map(x=> (x._1, x._2._1, x._2._2))
```

## Big data Immutability
* One of the key characteristics of data processing is that data is never modified in place. Instead, we apply operations that create new versions of the data, without modifying the original version.
* Immutability is a general concept that expands in much of the data processing stack.

![404]({{ site.url }}/images/bd/immutable.png)

### Copy-on-write (COW)
* Copy-On-Write is a general technique that allows us to share memory for read-only access accross processes and deal with writes only if/when they are performed by copying the modified resource in a private version.
* COW is the basis for many operating system mechanisms, such as process creation (forking), while many new filesystems (e.g. BTRFS, ZFS) use it as their storage format.
* COW enables systems with multiple readers and few writers to efficiently share resources.

### Immutable/persistent datastructures
* Immutable or persistent data structures always preserve the previous version of themselves when they are modified
* With immutable data structures, we can:
  * Avoid locking while processing them, so we can process items in parallel
  * Maintain and share old versions of data
  * Seamlessly persist the data on disk
  * Reason about changes in the data
* They come at a cost of increased memory usage (data is never deleted).

### Data structures in Scala

| ADT   | collection.mutable | collection.immutable |
| ----- | ------------------ | -------------------- |
| Array | ArrayBuffer        | Vector               |
| List  | LinkedList         | List                 |
| Map   | HashMap            | HashMap              |
| Set   | HashSet            | HashSet              |
| Queue | SynchronizedQueue  | Queue                |
| Tree  | —                  | TreeSet              |

## Why use Scala?
* It seems that FP is something that can be achieved with Java anyway right?
  * Java is the "assembly" of scala
  * FP is just a set of constraints on how to write code, which could be done within Java
* The reason to use Scala (and other FP languages) is that the syntactic sugar is optimized specifically for this type of programming philosophy/applications
  * You'll see that in big data applications 1 piece of data has to go over many transformations
    * Fetch
    * Clean
    * Transform 1
    * Transform 2
    * ...
    * Transform N
    * Export
    * ...
* Implementing a lot of (simple) transformations (in sperate methods) in Java would result in huge amounts of boilerplate code
* In scala you can apply lots of function compositions in a few lines (and just 1 file instead of having many files for the abstract classes):

```scala
type WebRequest = HttpRequest => HttpResponse
val = deSerialisePerson: HttpRequest => Person = ???
val createCustomer: Person => Customer = ???
val saveCustomer: Customer => Customer = ???
val serialiseCustomer: Customer => HttpResponse = ???

val registerCustomer: WebREquest =
  deserialise Person andThen
    createCustomer andThen
    saveCustomer andThen
    serialiseCustomer
```

* This syntax works very well for http requests, data pipelines, etc.
  * Because each function is completely independent
  * You can test them without mocking dependencies

## Distributed Systems
* A distributed system is a software system in which components located on networked computers communicate and coordinate their actions by passing messages.
  * Parallel systems use shared memory 
  * Distributed systems use no shared components
* Distriubted systems offer:
  * Scalability
    * Moore’s law: The number of transistors on a single chip doubles about every two year.
    * The advancement has slowed since around 2010.
    * Distribution provides massive performance.
  * Distribution of tasks and collaboration
  * Reduced latency
  * Fault tolerance
  * Mobility

### Characteristics
* Computational entities each with own memory
  * Need to synchronize distributed state
* Entities communicate with message passing
* Each entity maintains parts of the complete picture
* Need to tolerate failure
* They fail often (and failure is difficult to spot!)
  * Split-brain scenarios
* Maintaining order/consistency is hard
* Coordination is hard
* Partial operation must be possible
* Testing is hard
* Profiling is hard: “it’s slow” might be due to 1000s of factors

### Fallacies
* The network is reliable
* Latency is zero
* Bandwidth is infinite
* The network is secure
* Topology does not change
* Transport cost is zero
* The network is homogeneous

### Problems
* Partial failures: Some parts of the system may fail nondeterministically, while other parts work fine.
* Unreliable networks: Distributed systems communicate over unreliable networks.
* Unreliable time: Time is a universal coordination principle. However, we cannot use time determine order.
* No single source of truth, different opinions: Distributed systems need to co-ordinate and agree upon a (version of) truth.

### Asynchronous vs syncrhonous systems
* Synchronous system: Process execution speeds or message delivery times are bounded. In a synchronous system, we can have:
  * Timed failure detection
  * Time based coordination
  * Worst-case performance
* Asynchronous system: No assumptions about process execution speeds or message delivery times are made.
  * Upon waiting for a response to a requests in an asynchronous system, it is not possible to distinguish whether:
    * the request was lost
    * the remote node is down
    * the response was lost
  * Timeouts is a fundamental design choice in asynchronous networks: Ethernet, TCP and most application protocols work with timeouts.
* Purely synchronous systems only exist in theory.
* Most distributed systems use some form of asynchronous networking to communicate.


### Time is essential
* In a distributed system, time is the only global constant nodes can rely on to make distributed decisions on ordering problems.
* When do we need to order?
  * Sequencing items in memory
  * Mutual exclusion of access to a shared resource
  * Encoding history (“happens before” relationships)
  * Transactions in a database
  * Consistency of distributed data
  * Debugging (finding the root cause of a bug)
* Time in computers is kept in two ways:
  * “Real” time clocks (RTCs): Capture our intuition about time and are kept in sync with the NTP protocol with centralized servers. (e.g. System.getCurrentTimeMillis).
    * (Synced time from online server)
  * Monotonic clocks: Those clocks only move forward. (e.g. System.nanoTime)
    * (Harward time mantained by the OS)
    * They are (usually!) good for determining order within a node, but each node only has its own notion of time.

#### Logical time
* Idea: Instead of using the precise clock time, capture the events relationship between a pair of events
* Based on causality: If some event possibly causes another event, then the first event happened-before the other
* Order: A way of arranging items in a set so that the following properties are maintained.
  * Strict partial order:
    * `Irreflexivity: ∀a.¬a<a (items not comparable with self)`
    * `Transitivity: if a≤b and b≤c then a≤c`
    * `Antisymmetry: if a≤b and b≤a a=b`
  * Strict total order:
    * `An additional property: ∀a,b,a≤b∨b≤a∨a=b`
* Lamport introduced happens-before relation to capture dependencies between events
  * It is a strict partial order: it is irreflexive, antisymmetric and transitive.
* Two events not related to happened-before are concurrent.






