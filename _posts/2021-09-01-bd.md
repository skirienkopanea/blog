---
layout: post
title:  "Big Data Processing summary"
date:   2021-09-01 00:51:00 +0200
categories: datascience
tags: CSE2520
---
{% include math.html %}
<!--more-->

This is a summary of CSE2520 Big Data Processing

# Table of Contents
- [Table of Contents](#table-of-contents)
  - [Big and Fast Data](#big-and-fast-data)
    - [What is big data?](#what-is-big-data)
    - [How big is "big"?](#how-big-is-big)
    - [Vs of Big data:](#vs-of-big-data)
      - [Volume](#volume)
      - [Variety](#variety)
      - [Velocity](#velocity)
  - [Big data processing](#big-data-processing)
    - [ETL cycle](#etl-cycle)
    - [Big data engenieering](#big-data-engenieering)
    - [Big data analytics](#big-data-analytics)
    - [Batch processing](#batch-processing)
    - [Stream processing](#stream-processing)
    - [Data processing distribution](#data-processing-distribution)
    - [Desired properties of a big data processing system](#desired-properties-of-a-big-data-processing-system)
    - [Large-scale computing](#large-scale-computing)
    - [Big Data tech timeline](#big-data-tech-timeline)
    - [Current Big Data tech landscape](#current-big-data-tech-landscape)
  - [Problems solved with Big Data](#problems-solved-with-big-data)
  - [Big Data Programming Languages](#big-data-programming-languages)
  - [Hello world](#hello-world)

## Big and Fast Data
### What is big data?
* Besides a buzzword is also used to describe:
  * *Data too large to be efficiently processed on a **single computer***
  * *Massive amounts of **diverse**, **unstructured** data produced by **high-performance applications***

### How big is "big"?
* Typical numbers associated with big data:
  * 2.5 Exabytes (\\(2.5 \cdot 10^6 TB\\)) produced daily
  * IoT: 21.5 billion devices with internet access
  * Facebook, Amazon, Microsoft and Google store at least 1,200 petabytes of information
  * 100k google seraches per second
    * each query involves more than 1k machines
    * each query search touches more than 200 services
  * Amazon processes more than 1k orders per second
  * 1 billion of daily instagram users

### Vs of Big data:
Main Vs:

* Volume
* Variety (different forms and sources)
* Velocity (content changes quickly)

Other:

* (Business) value
* Veracity (accuracy)
* Validity (interpretation)
* Visibility
* Volability
* Virality

#### Volume
* We call big data big because of the volume
  * 90% of all data ever was created in the last 2 years
  * The global big data and business analytics market was valued at 138.9 billion in 2020 and is expected to grow

![404]({{ site.url }}/images/bd/growth.png)

#### Variety
* Structured data: SQL tables, images, format is known
* Semi-structured data: JSON, XML
* Unstructured data: Text

#### Velocity
* Big data is not just big (volume) (and varied), but it's also generated and processed fast:
  * Data centers write a lot to log files
  * Social media posts
  * Stock market high-frequency trading (latency costs money)
  * Online advertising
* Data needs to be processed with soft or hard real-time guarantees

## Big data processing
### ETL cycle
* Extract: Convert raw or semi-structured data into structured data (i.e. JSON to database tables)
* Transform: Convert units, join data sources, clean data...
* Load: load the data into another system for further processing

### Big data engenieering
* It's about building "pipelines"

### Big data analytics
* It's about discovering patterns

### Batch processing
* All data exists in some data store, a program processes the whole dataset at once (i.e. FRISS csv historical fraud batches)

### Stream processing
* Processing of data as they arrive to the system (i.e. FRISS real time fraud score)

### Data processing distribution
* Divide the data (i.e. csv of historical fraud) in chunks and apply the same task on all chunks at the same time, i.e. via multiple machines/CPUs with each machine assigned with it's unique chunk (data-parallelism: one task, many data splits)
* If possible, divide the task into independent sub-tasks that use the same data source (i.e. replace(",",".") for all records in a column and at the same time replace blanks with "0.0")

### Desired properties of a big data processing system
* Robustness and fault-tolerance
* Low latency reads and updates
* Scalability
* Generalization
* Extensibility
* Ad hoc queries
* Minimal maintenance
* Debuggability

### Large-scale computing
* Emerged in the 70's
* Phisicists used super computers for simulations in the 80's
* Shared-memory designs are still in large scale use
* What's new is: Large scale processing on **distributed**, **commodity** computers (i.e. average linux user home computer) enabled by advanced software using **elastic** resource allocation
* It is **software** and not hardware what drives the Big Data industry

### Big Data tech timeline
Progress is mostly industry-driven:

* 2003: Google publishes the Google Filesystem paper, a large-scale distributed file system
* 2004: Google publishes the Map/Reduce paper, a distributed data processing abstraction
* 2006: Yahoo creates and open sources Hadoop, inspired by the Google papers
* 2006: Amazon launches its Elastic Compute Cloud, offering cheap, elastic resources
* 2007: Amazon publishes the DynamoDB paper, sketches the blueprints of a cloud-native database
* 2009 â€“ onwards: The NoSQL movement. Schema-less, distributed databases defy the SQL way of storing data
* 2010: Matei Zaharia et al. publish the Spark paper, brings FP to in-memory computations
* 2012: Both Spark Streaming and Apache Flink appear, able to handle really high volume stream processing
* 2012: Alex Krizhevsky et al. publish their deep learning image classification paper re-igniting interest in neural networks and solidifying the value of big data


### Current Big Data tech landscape
![404]({{ site.url }}/images/bd/landscape.png)

## Problems solved with Big Data
* Modelling: What factors influence particular outcomes/behaviour?
* Inormation retrivals: Search engines, web scrappers
* Collaborative filtering: Recommending items based on items other users with similar tests ahve chosen
* Outlier detection: Discovering outstanding transactions

## Big Data Programming Languages
* The Big data and data science languages are
  * **scala**: for intensive systems
    * Strong point is the combination of functional programming and object oriented programming
  * **python**: for data analytics tasks
    * Strong point is the combination of object oriented and imperative programming
  * Both support object oriented programming, functional programming and imperative programming. But python is interpreted and scala is combined
* Other languages include
  * **Java**: the language in which most big data infrastructure is written into
  * **R**: Statistics language with great selection of libraries for serious data analytics and plotting tools


## Hello world
* Scala is not only similar to java, but it can also actually run java code itself
  * Both Scala and Java are compiled to JVM bytecode
  * Scala can interoperate with JVM libraries
  * Scala is not sensitive to spaces/tabs. Blocks are denoted by `{}`

```scala
object Hello extends App {
    println("Hello, world")
    for (i <- 1 to 10) {
      System.out.println("Hello")
    }
}
```

* Hello world in Python

```py
#!/usr/bin/env python3

for i in range(1, 10):
  print("Hello, world")
```


