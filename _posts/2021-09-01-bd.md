---
layout: post
title:  "Big Data Processing summary"
date:   2021-09-01 00:51:00 +0200
categories: datascience
tags: CSE2520
---
{% include math.html %}
<!--more-->

This is a summary of CSE2520 Big Data Processing

# Table of Contents
- [Table of Contents](#table-of-contents)
  - [Big and Fast Data](#big-and-fast-data)
    - [What is big data?](#what-is-big-data)
    - [How big is "big"?](#how-big-is-big)
    - [Vs of Big data:](#vs-of-big-data)
      - [Volume](#volume)
      - [Variety](#variety)
      - [Velocity](#velocity)
  - [Big data processing](#big-data-processing)
    - [ETL cycle](#etl-cycle)
    - [Big data engenieering](#big-data-engenieering)
    - [Big data analytics](#big-data-analytics)
    - [Batch processing](#batch-processing)
    - [Stream processing](#stream-processing)
    - [Data processing distribution](#data-processing-distribution)
    - [Desired properties of a big data processing system](#desired-properties-of-a-big-data-processing-system)
    - [Large-scale computing](#large-scale-computing)
    - [Big Data tech timeline](#big-data-tech-timeline)
    - [Current Big Data tech landscape](#current-big-data-tech-landscape)
  - [Problems solved with Big Data](#problems-solved-with-big-data)
  - [Big Data Programming Languages: Scala & Python](#big-data-programming-languages-scala--python)
    - [Hello world](#hello-world)
    - [Declarations](#declarations)
    - [Declaring functions](#declaring-functions)
    - [Declaring classes](#declaring-classes)
    - [Inheritance](#inheritance)
    - [Data classes](#data-classes)
    - [Pattern matching in Scala](#pattern-matching-in-scala)

## Big and Fast Data
### What is big data?
* Besides a buzzword is also used to describe:
  * *Data too large to be efficiently processed on a **single computer***
  * *Massive amounts of **diverse**, **unstructured** data produced by **high-performance applications***

### How big is "big"?
* Typical numbers associated with big data:
  * 2.5 Exabytes (\\(2.5 \cdot 10^6 TB\\)) produced daily
  * IoT: 21.5 billion devices with internet access
  * Facebook, Amazon, Microsoft and Google store at least 1,200 petabytes of information
  * 100k google seraches per second
    * each query involves more than 1k machines
    * each query search touches more than 200 services
  * Amazon processes more than 1k orders per second
  * 1 billion of daily instagram users

### Vs of Big data:
Main Vs:

* Volume
* Variety (different forms and sources)
* Velocity (content changes quickly)

Other:

* (Business) value
* Veracity (accuracy)
* Validity (interpretation)
* Visibility
* Volability
* Virality

#### Volume
* We call big data big because of the volume
  * 90% of all data ever was created in the last 2 years
  * The global big data and business analytics market was valued at 138.9 billion in 2020 and is expected to grow

![404]({{ site.url }}/images/bd/growth.png)

#### Variety
* Structured data: SQL tables, images, format is known
* Semi-structured data: JSON, XML
* Unstructured data: Text

#### Velocity
* Big data is not just big (volume) (and varied), but it's also generated and processed fast:
  * Data centers write a lot to log files
  * Social media posts
  * Stock market high-frequency trading (latency costs money)
  * Online advertising
* Data needs to be processed with soft or hard real-time guarantees

## Big data processing
### ETL cycle
* Extract: Convert raw or semi-structured data into structured data (i.e. JSON to database tables)
* Transform: Convert units, join data sources, clean data...
* Load: load the data into another system for further processing

### Big data engenieering
* It's about building "pipelines"

### Big data analytics
* It's about discovering patterns

### Batch processing
* All data exists in some data store, a program processes the whole dataset at once (i.e. FRISS csv historical fraud batches)

### Stream processing
* Processing of data as they arrive to the system (i.e. FRISS real time fraud score)

### Data processing distribution
* Divide the data (i.e. csv of historical fraud) in chunks and apply the same task on all chunks at the same time, i.e. via multiple machines/CPUs with each machine assigned with it's unique chunk (data-parallelism: one task, many data splits)
* If possible, divide the task into independent sub-tasks that use the same data source (i.e. replace(",",".") for all records in a column and at the same time replace blanks with "0.0")

### Desired properties of a big data processing system
* Robustness and fault-tolerance
* Low latency reads and updates
* Scalability
* Generalization
* Extensibility
* Ad hoc queries
* Minimal maintenance
* Debuggability

### Large-scale computing
* Emerged in the 70's
* Phisicists used super computers for simulations in the 80's
* Shared-memory designs are still in large scale use
* What's new is: Large scale processing on **distributed**, **commodity** computers (i.e. average linux user home computer) enabled by advanced software using **elastic** resource allocation
* It is **software** and not hardware what drives the Big Data industry

### Big Data tech timeline
Progress is mostly industry-driven:

* 2003: Google publishes the Google Filesystem paper, a large-scale distributed file system
* 2004: Google publishes the Map/Reduce paper, a distributed data processing abstraction
* 2006: Yahoo creates and open sources Hadoop, inspired by the Google papers
* 2006: Amazon launches its Elastic Compute Cloud, offering cheap, elastic resources
* 2007: Amazon publishes the DynamoDB paper, sketches the blueprints of a cloud-native database
* 2009 â€“ onwards: The NoSQL movement. Schema-less, distributed databases defy the SQL way of storing data
* 2010: Matei Zaharia et al. publish the Spark paper, brings FP to in-memory computations
* 2012: Both Spark Streaming and Apache Flink appear, able to handle really high volume stream processing
* 2012: Alex Krizhevsky et al. publish their deep learning image classification paper re-igniting interest in neural networks and solidifying the value of big data


### Current Big Data tech landscape
![404]({{ site.url }}/images/bd/landscape.png)

## Problems solved with Big Data
* Modelling: What factors influence particular outcomes/behaviour?
* Inormation retrivals: Search engines, web scrappers
* Collaborative filtering: Recommending items based on items other users with similar tests ahve chosen
* Outlier detection: Discovering outstanding transactions

## Big Data Programming Languages: Scala & Python
* The Big data and data science languages are
  * **scala**: for intensive systems
    * Strong point is the combination of functional programming and object oriented programming
    * [documentation](https://docs.scala-lang.org/)
  * **python**: for data analytics tasks
    * Strong point is the combination of object oriented and imperative programming
    * [documentation](https://docs.python.org/2/)
  * Both support object oriented programming, functional programming and imperative programming. But python is interpreted and scala is combined
* Other languages include
  * **Java**: the language in which most big data infrastructure is written into
  * **R**: Statistics language with great selection of libraries for serious data analytics and plotting tools


### Hello world
* Scala is not only similar to java, but it can also actually run java code itself
  * Both Scala and Java are compiled to JVM bytecode
  * Scala can interoperate with JVM libraries
  * Scala is not sensitive to spaces/tabs. Blocks are denoted by `{}`

```scala
object Hello extends App {
    println("Hello, world")
    for (i <- 1 to 10) {
      System.out.println("Hello")
    }
}
```

* Hello world in Python
  * Python is interpreted
  * Python is indentation sensitive: blocks are denoted by a TAB or 2 spaces.

```py
for i in range(1, 10):
  print("Hello, world")
```

### Declarations
* Scala:
  * Type inference used extensively (no need to explicitly declare the type of the variable like in js, but you may do it)
  * Two types of variables: vals are constants, vars are variables

```scala
object Declarations extends App {
  var a: Int = 5
  val b = 6

  println(a)

  //b = 6 // wont compile program because val is like js' "const"

  println(b)

  // Type of foo is inferred
  val foo = new Array[Int](5)

  // var a = "Foo" // wont compile because a is already defined with val
  var c = "Foo"
  // c = 42 // won't compile due to type mismatch
  c = "Bar"

  print(c)
}
```

* python:
  * Optional typing, not enforced at runtime

```py
import numpy as np

a: int = 5
a = "Foo"

a = np.array([a, 6, 7, 8])

print(a)
```
`['Foo' '6' '7' '8']`

### Declaring functions
* Scala:
  * Statically typed
  * Evaluated expressions have types
  * The return type is the most generic type of all return expressions

```scala
object Functions extends App {
 println(max(3,1))

  def max(x: Int, y: Int): Int =
    if (x >= y) x else y
}
```

* Python:
  * Dynamically typed
  * Mostly based on statements
  * Types are optional


```py
def maxi(x: int, y: int) -> int:
    if x >= y:
        return x
    else:
        return y


print(maxi(5, 3))  # you cant use functions before they're defined since python is executed line by line
```

### Declaring classes
* Scala
  * A default constructor is created automatically

```scala
class ClassExample(
                    val x: Int,
                    var y: Double = 0.0
                  )
```
```scala
  // Type of a is inferred
  val a = new ClassExample(1, 4.0)
  println(a.x) //x is read-only
  println(a.y) //y is read-write
  a.y = 10.0
  println(a.y) //y is read-write
```
```
1
4.0
10.0
```

* Python

```py
class Foo:
    def __init__(self, x, y):
        self.x = x
        self.y = y
```
```py
import class_example as ce

a = ce.Foo(3, 2)
print(a.x)

a.x = "foo"
print(a.x)

```
```
3
foo
```

### Inheritance
* Scala
  * Traits are equivalent to java interfaces (abstract classes (cant be initiliazed itself) whose methods don't have body) and includes attributes

```scala
object Inheritance extends App {
  var c = new Baz(5, 6f, 7)
  println(c.asString())
}


class Foo(val x: Int,
          var y: Double = 0.0)

class Bar(x: Int, y: Int, z: Int)
  extends Foo(x, y)

trait Printable {
  val s: String

  def asString(): String
}

class Baz(x: Int, y: Double, private val z: Int)
  extends Foo(x, y) with Printable {
  override val s: String = new String( //java code
    String.valueOf(x)
      + " "
      + String.valueOf(y)
      + " "
      + String.valueOf(z))

  override def asString(): String = s
}
```
`5 6.0 7`

* Python

```py
class TwoDimensionPoint:
    def __init__(self, x, y):
        self.x = x
        self.y = y


class ThreeDimensionPoint(TwoDimensionPoint):
    def __init__(self, x, y, z):
        TwoDimensionPoint.__init__(self, x, y)
        self.z = z


a = TwoDimensionPoint(1, 2)
b = ThreeDimensionPoint(10, 20, 30)

print(a.x, a.y)
print(b.x, b.y, b.z)
```
```
1 2
10 20 30
```

* In both scala and python children can override parents

### Data classes
* Data classes are blueprints for immutable objects.
* We use them to represent data records.
* Both languages implement equals (or __eq__) for them, so we can compare objects directly.

* Scala:

```scala
object DataClass extends App {
  val p = Person("Name", Address("Street", 2))
  // p.name = "sergio" //wont compile

  println(new String(p.name + " lives at " + p.address.street + " " + p.address.number))
}

case class Address(street: String,
                   number: Int)

case class Person(name: String,
                  address: Address)
```
`Name lives at Street 2`

* Python:

```py
from dataclasses import dataclass


@dataclass
class Address:
    street: str
    number: int


@dataclass
class Person:
    name: str
    address: Address


p = Person("G", Address("a", 2))
p.name = "Sergio"  # does compile
print(p)
```
`Person(name='Sergio', address=Address(street='a', number=2))`

### Pattern matching in Scala
* It's the equivalent of switch in java

* Java:

```java
public String getInstruction() {
        switch (instruction) {
            case LDA:
                return "0001";
            case ADD:
                return "0010";
            case SUB:
                return "0011";
            case STA:
                return "0100";
            case LDI:
                return "0101";
            case JMP:
                return "0110";
            case JPC:
                return "0111";
            case JPZ:
                return "1000";
            case OUT:
                return "1110";
            case HLT:
                return "1111";
            default:
                return "0000";
        }
    }
```

* Scala:

```scala
object PatternMatching extends App {
  val instruction = "LDA"

  def getInstruction(opcode: String): String =
    instruction match {
      case "LDA" => "0001"
      case "ADD" => "0010"
      case "SUB" => "0011"
      case "STA" => "0100"
      case "LDI" => "0101"
      case "JMP" => "0110"
      case "JPC" => "0111"
      case "JPZ" => "1000"
      case "OUT" => "1110"
      case "HLT" => "1111"
      case _ => "0000"
    }

  println(getInstruction(instruction))
  
}
```
`0001`

