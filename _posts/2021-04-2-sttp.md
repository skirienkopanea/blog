---
layout: post
title:  "Software Quality and Testing Summary"
date:   2021-04-10 00:51:00 +0200
categories: testing
tags: CSE1110
---
{% include math.html %}
<!--more-->

# Table of Contents
- [Table of Contents](#table-of-contents)
  - [Terminology](#terminology)
    - [Small checks](#small-checks)
  - [Principles of software testing](#principles-of-software-testing)
  - [Test Design](#test-design)
    - [Test levels](#test-levels)
    - [Test types](#test-types)
  - [JUnit](#junit)
    - [@BeforeEach](#beforeeach)
  - [@ParameterizedTest](#parameterizedtest)
  - [AAA Automated tests: Arrange, Act, Assert](#aaa-automated-tests-arrange-act-assert)
    - [Advantages of test automation](#advantages-of-test-automation)
  - [Testing techniques](#testing-techniques)
    - [Specification based testing](#specification-based-testing)
      - [Partioning the input space](#partioning-the-input-space)
      - [Equivalence partionaing](#equivalence-partionaing)
    - [Category-Partition method](#category-partition-method)
  - [Boundary testing](#boundary-testing)
    - [On and Off points](#on-and-off-points)
    - [Automating boundary testing with @CsvSource](#automating-boundary-testing-with-csvsource)
    - [The CORRECT way for boundary testing](#the-correct-way-for-boundary-testing)
    - [Domain testing](#domain-testing)

## Terminology
* **Failure:** Manifested inability of a system to performa required function. (When a system stops working as expected)
  * **Defect, fault, bug**: The cause of the failure in terms of code/hardware implementation.
    * **Error (mistake)**: The cause of the bug (i.e. developer negligency)
* **Testing:** attempt to trigger failures
  * **Debugging**: attempt to find the failure bug (defect, fault)
* **Verification:** Verify that the system behaves bug free
* **Validation:** Validate that the system delivers the busines value it should deliver (features)

### Small checks
A common technique developers use (which we will try as much as possible to convince you not to do) is that they implement the program based on the requirements, and then perform "small checks" to make sure the program works as expected. However these checks only 

## Principles of software testing
1. Testing cannot show absence of bugs:
   * Abscence of evidence is not evidence of absence.
   * Testing specific scenarios only ensures those scenarios behave as expected. 
2. Exaustive testing is impossible:
   * Possible scenarios increase exponentially as features are added. In a moderately large program, it's impossible to test them all, but since **bugs are not uniformly distributed** we should focus on finding the bug-prone areas.
3. Testing needs to start early
4. Defects tend to be clustered
5. Pesticide paradox yields test methods ineffective:
   * Applying the same techniques over and over yields diminishing returns as you are leaving other types of bugs untested
   * There is no one silver bullet testing strategy that can guarantee a complete bug-free software. Combining different testing strategies yields a better result in finding bugs. 
6. Testing is context-dependent
   * A mobile app needs diffferent tests than a web app 
7. There is more to quality than absence of defects
   * Besides software verification (bug free) we need software validation to ensure business value.

## Test Design
* Decide which of the inintely many possible test cases to create
  * Maximize information gain
  * Minimize cost
* **test strategy:** Systematic approach to reach test cases
  * targets specific types of faults until a given **adequacy criterion** is achieved
* Test design begins at the start of the project

### Test levels
Have different levels of granularity:
* Unit testing
* Integration testing
* System testing
* ...

### Test types
Different objectives:
* Functionality (old/new)
* Security
* Performance
* ...

## JUnit
The steps to create a JUnit class/test is often the following:

* Create a Java class under the directory /src/test/java/roman/ (or whatever test directory your project structure uses). As a convention, the name of the test class is similar to the name of the class under test. For example, a class that tests the RomanNumeral class is often called RomanNumeralTest. In terms of package structure, the test class also inherits the same package as the class under test.

* For each test case we devise for the program/class, we write a test method. A JUnit test method returns void and is annotated with @Test (an annotation that comes from JUnit 5's org.junit.jupiter.api.Test). The name of the test method does not matter to JUnit, but it does matter to us. A best practice is to name the test after the case it tests.

* The test method instantiates the class under test and invokes the method under test. The test method passes the previously defined input in the test case definition to the method/class. The test method then stores the result of the method call (e.g., in a variable).

* The test method asserts that the actual output matches the expected output. The expected output was defined during the test case definition phase. To check the outcome with the expected value, we use assertions. An assertion checks whether a certain expectation is met; if not, it throws an AssertionError and thereby causes the test to fail. A couple of useful assertions are:
  * Assertions.assertEquals(expected, actual): Compares whether the expected and actual values are equal. The test fails otherwise. Be sure to pass the expected value as the first argument, and the actual value (the value that comes from the program under test) as the second argument. Otherwise the fail message of the test will not make sense.
  * Assertions.assertTrue(condition): Passes if the condition evaluates to true, fails otherwise.
  * Assertions.assertFalse(condition): Passes if the condition evaluates to false, fails otherwise.
  * More assertions and additional arguments can be found in JUnit's documentation. To make easy use of the assertions and to import them all in one go, you can use import static org.junit.jupiter.api.Assertions.*;.

```java
import static org.junit.jupiter.api.Assertions.*;
import org.junit.jupiter.api.Test;

public class RomanNumeralTest {

  @Test
  void convertSingleDigit() {
    RomanNumeral roman = new RomanNumeral();
    int result = roman.convert("C");

    assertEquals(100, result);
  }

  @Test
  void convertNumberWithDifferentDigits() {
    RomanNumeral roman = new RomanNumeral();
    int result = roman.convert("CCXVI");

    assertEquals(216, result);
  }

  @Test
  void convertNumberWithSubtractiveNotation() {
    RomanNumeral roman = new RomanNumeral();
    int result = roman.convert("XL");

    assertEquals(40, result);
  }
}
```
### @BeforeEach
* JUnit runs methods that are annotated with @BeforeEach before every test method to avoid code duplication.

```java
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;

import static org.junit.jupiter.api.Assertions.*;

class RomanNumeralTest {

  private RomanNumeral roman;

  @BeforeEach
  void setup() {
    roman = new RomanNumeral();
  }

  @Test
  void convertSingleDigit() {
    int result = roman.convert("C");
    assertEquals(100, result);
  }

  @Test
  void convertNumberWithDifferentDigits() {
    int result = roman.convert("CCXVI");
    assertEquals(216, result);
  }

  @Test
  void convertNumberWithSubtractiveNotation() {
    int result = roman.convert("XL");
    assertEquals(40, result);
  }
}
```
## @ParameterizedTest
* We write a generic test method whose values are generated in runtime by the parameters of such method.
  * To feed those values we define a source with `@MethodSource("generatorMethodName")`
  * `private static Stream<Arguments> generator()` returns a `Stream.of(arguments)` that will be inserted as paramaters in the parameterized test method.
    * The arugments must have the same number of elements as the paramaterized method
    * The parameterized method will be run each time for each stream of arguments sent

```java
import org.junit.jupiter.params.ParameterizedTest;
import org.junit.jupiter.params.provider.Arguments;
import org.junit.jupiter.params.provider.MethodSource;

import java.util.stream.Stream;

import static org.junit.jupiter.api.Assertions.*;

class NumFinderTest {

    // This one line instantiation in practice is run each time before each test
    private final NumFinder n = new NumFinder();

    @ParameterizedTest
    @MethodSource("generator")
    void getMinMax(int[] nums, int expectedMin, int expectedMax) {
        n.find(nums);

        assertEquals(expectedMax, n.getLargest());
        assertEquals(expectedMin, n.getSmallest());
    }

    private static Stream<Arguments> generator() {
        Arguments tc1 = Arguments.of(new int[]{27, 26, 25}, 25, 27);
        Arguments tc2 = Arguments.of(new int[]{5, 2, 15, 27}, 2, 27);
        return Stream.of(tc1, tc2);
    }
}
```

## AAA Automated tests: Arrange, Act, Assert
* **Arrange:** Define the input values that will be passed onto the automated test
* **Act:** Pass the input values by means of one or more method calls
* **Assert:** Execute assert instructions

```java
@Test
void convertSingleDigit() {
  // Arrange: we define the input values
  String romanToBeConverted = "C";

  // Act: we invoke the method under test
  int result = roman.convert(romanToBeConverted);

  // Assert: we check whether the output matches the expected result
  assertEquals(100, result);
}
```
### Advantages of test automation
As in comparision to manually checking System.out.println...
* Less prone to human mistakes
* Faster than developers
* Efficient refactoring (you can change the code without having to change the tests)

## Testing techniques
### Specification based testing
* These **use the requirements of the program** (often written as text; think of user stories and/or UML use cases) as input for testing.
* these techniques are also referred to as black box testing as they dont need you to know details such as in which software the program was developed or which data structures are used in the implementation

#### Partioning the input space
* Based on the requirements -> create paritions that represent each possible case -> create unit test

#### Equivalence partionaing
* The idea that inputs being equivalent to each other yield the same outcome (i.e. testing for multiples of 4 can take 4q with any q)
  * Chosing 1 of the equivalent elements is more than enough

### Category-Partition method
* Systematic way of deriving test cases, **based on the characteristics of the input parameters**.
* reduces the number of tests to a practical number

1. Identify the parameters, or the input for the program. For example, the parameters your classes and methods receive.
2. Derive characteristics of each parameter. For example, an int year should be a positive integer number between 0 and infinite. 
   1. Some of these characteristics can be found directly in the specification of the program.
   2. Others might not be found from specifications. For example, an input cannot be null if the method does not handle that well.
3. Add constraints in order to minimise the test suite.
4. Generate combinations of the input values. (Like a cartesian product)These are the test cases.

## Boundary testing
* Test that regard edge cases
* This boundaries regard the boundaries of test partitions
* we can find such boundaries by finding a pair of consecutive input values [p1,p2], where p​1​​ belongs to partition A, and p​2​​ belongs to partition B.
* However, in longer conditions, full of boundaries, the number of combinations might be too high, making it unfeasible for the developer to test them all.

### On and Off points
* On-point: The on-point is the value that is exactly on the boundary. This is the value we see in the condition itself.
  * Note that, depending on the condition, an on-point can be either an in- or an out-point.
  * some authors argue that testing boundaries is enough. If the number of test cases is indeed too high, and it is just too expensive to do them all, prioritization is important, and we suggest testers to indeed focus on the boundaries.
* Off-point: The off-point is the value that is closest to the boundary and that flips the condition. If the on-point makes the condition true, the off point makes it false and vice versa. Note that when dealing with equalities or inequalities (e.g. x=6x = 6x=6 or x≠6x \neq 6x≠6), there are two off-points; one in each direction.
* In-points: In-points are all the values that make the condition true.
* Out-points: Out-points are all the values that make the condition false.

The problem is that this boundary is just less explicit from the requirements. Boundaries also happen when we are going from "one partition" to another. There is a "single condition" that we can use as clear source. In these cases, what we should do is to devise test cases for a sequence of inputs that move from one partition to another.

### Automating boundary testing with @CsvSource
* The CsvSource expects list of strings, where each string represents the input and output values for one test case.

```java
@ParameterizedTest(name = "small={0}, big={1}, total={2}, result={3}")
    @CsvSource({
      // The total is higher than the amount of small and big bars.
      "1,1,5,0", "1,1,6,1", "1,1,7,-1", "1,1,8,-1",
      // No need for small bars.
      "4,0,10,-1", "4,1,10,-1", "5,2,10,0", "5,3,10,0",
      // Need for big and small bars.
      "0,3,17,-1", "1,3,17,-1", "2,3,17,2", "3,3,17,2",
      "0,3,12,-1", "1,3,12,-1", "2,3,12,2", "3,3,12,2",
      // Only small bars.
      "4,2,3,3", "3,2,3,3", "2,2,3,-1", "1,2,3,-1"
    })
    void boundaries(int small, int big, int total, int expectedResult) {
        int result = new ChocolateBars().calculate(small, big, total);
        Assertions.assertEquals(expectedResult, result);
    }
```

which is the same as the @ParameterizedTest with a @MethodSource:

```java
public class ChocolateBarsTest {

    @ParameterizedTest(name = "small={0}, big={1}, total={2}, result={3}")
    @MethodSource("generator")
    void boundaries(int small, int big, int total, int expectedResult) {
        int result = new ChocolateBars().calculate(small, big, total);
        Assertions.assertEquals(expectedResult, result);
    }

    private static Stream<Arguments> generator() {
      return Stream.of(
        // The total is higher than the amount of small and big bars.
        Arguments.of(1,1,5,0),
        Arguments.of(1,1,6,1),
        Arguments.of(1,1,7,-1),
        Arguments.of(1,1,8,-1),
        // No need for small bars.
        Arguments.of(4,0,10,-1),
        Arguments.of(4,1,10,-1),
        Arguments.of(5,2,10,0),
        Arguments.of(5,3,10,0),
        // Need for big and small bars.
        Arguments.of(0,3,17,-1),
        Arguments.of(1,3,17,-1),
        Arguments.of(2,3,17,2),
        Arguments.of(3,3,17,2),
        Arguments.of(0,3,12,-1),
        Arguments.of(1,3,12,-1),
        Arguments.of(2,3,12,2),
        Arguments.of(3,3,12,2),
        // Only small bars.
        Arguments.of(4,2,3,3),
        Arguments.of(3,2,3,3),
        Arguments.of(2,2,3,-1),
        Arguments.of(1,2,3,-1)
      );

    }
}
```
### The CORRECT way for boundary testing
* Conformance
  * Test what happens when your **input** is not in conformance with what is expected. i.e. string instead of int, not an email, etc.
* Ordering
  * Test different input order (i.e. sometimes the method only worked for sorted arrays)
* Range
  *  Test what happens when we provide inputs that are outside of the expected range. (i.e. negative numbers for age)
* Refference (for OOP methods)
  * What it references outside its scope
  * What external dependencies it has
  * Whether it depends on the object being in a certain state
  * Any other conditions that must exist
* Existence
  * Does the system behave correctly when something that is expected to exist, does not? i.e. null pointer errors
* Cardinality
  * Test loops in different situations, such as when it actually performs zero iterations, one iterations, or many. 
* Time
  * What happens if the system receives inputs that are not ordered in regards to date and time?
  * Timing of successive events
  * Does the system handle timeouts well?
  * Does the system handle concurrency well? (multiple computations are happening at the same time.)
  * Time formats and time zones

### Domain testing
* combination of equivalent class analysis and boundary testing


1. We read the requirement
2. We identify the input and output variables in play, together with their types, and their ranges.
3. We identify the dependencies (or independence) among input variables, and how input variables influence the output variable.
4. We perform equivalent class analysis (valid and invalid classes).
5. We explore the boundaries of these classes.
6. We think of a strategy to derive test cases, focusing on minimizing the costs while maximizing fault detection capability.
7. We generate a set of test cases that should be executed against the system under test.
